# -*- coding: utf-8 -*-
"""ventilation_wrangling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VxsuJP3FJhc9Q0_GSSKSRG9b7EnrdUn8

# MIMIC-IV Data Wrangling for Ventilation Prediction
This notebook processes MIMIC-IV data for ventilation prediction machine learning models. It:
- Splits admissions into 6-hour ICU periods.
- Labels ventilation status (PEEP ≥ 5, FiO2 ≥ 21 and record of ventilation procedure 225792).
- Downsamples training data to a 5:1 non-vent:vent ratio.
- Adds features: lab events, vitals, diagnoses, notes, demographics.
- Generates time-series (for LSTM) and aggregated (mean, min, max, slope) datasets.
"""

import pandas as pd
import numpy as np
import pickle
import ast
import argparse
import os
from scipy.stats import linregress

# Function to load in MIMIC-IV data extracted in ventilation_data_collection file
def parse_args():
    parser = argparse.ArgumentParser(description="Process MIMIC-IV data for ventilation prediction.")
    parser.add_argument('--input_dir', type=str, default='data/raw/',
                       help='Directory containing raw MIMIC-IV CSV files.')
    parser.add_argument('--output_dir', type=str, default='data/wrangled/',
                       help='Directory to save processed time-series and aggregated CSVs.')
    return parser.parse_args()

# For notebook, set manually (replace with parse_args() in .py)
args = type('Args', (), {
    'input_dir': 'data/raw/',
    'output_dir': 'data/wrangled/'
})()

# Load data
train_patients_df = pd.read_csv(os.path.join(args.input_dir, 'train_patients.csv'))
test_patients_df = pd.read_csv(os.path.join(args.input_dir, 'test_patients.csv'))
train_notes_df = pd.read_csv(os.path.join(args.input_dir, 'radiology_train_notes.csv'))
test_notes_df = pd.read_csv(os.path.join(args.input_dir, 'radiology_test_notes.csv'))
train_admissions_df = pd.read_csv(os.path.join(args.input_dir, 'train_admissions.csv'))
test_admissions_df = pd.read_csv(os.path.join(args.input_dir, 'test_admissions.csv'))
train_procedures_df = pd.read_csv(os.path.join(args.input_dir, 'train_procedures.csv'))
test_procedures_df = pd.read_csv(os.path.join(args.input_dir, 'test_procedures.csv'))
train_labevents_df = pd.read_csv(os.path.join(args.input_dir, 'train_labevents.csv'))
test_labevents_df = pd.read_csv(os.path.join(args.input_dir, 'test_labevents.csv'))
train_chartevents_df = pd.read_csv(os.path.join(args.input_dir, 'train_chartevents.csv'))
test_chartevents_df = pd.read_csv(os.path.join(args.input_dir, 'test_chartevents.csv'))
train_inputevents_df = pd.read_csv(os.path.join(args.input_dir, 'train_inputevents.csv'))
test_inputevents_df = pd.read_csv(os.path.join(args.input_dir, 'test_inputevents.csv'))
train_diagnoses_df = pd.read_csv(os.path.join(args.input_dir, 'train_diagnoses.csv'))
test_diagnoses_df = pd.read_csv(os.path.join(args.input_dir, 'test_diagnoses.csv'))
train_icustays_df = pd.read_csv(os.path.join(args.input_dir, 'train_icustays.csv'))
test_icustays_df = pd.read_csv(os.path.join(args.input_dir, 'test_icustays.csv'))

train_patients_df = pd.read_csv('/content/drive/MyDrive/ventilation-prediction-data/raw/train_patients.csv')
test_patients_df = pd.read_csv('/content/drive/MyDrive/ventilation-prediction-data/raw/test_patients.csv')
train_notes_df = pd.read_csv('/content/drive/MyDrive/ventilation-prediction-data/raw/radiology_train_notes.csv')
test_notes_df = pd.read_csv('/content/drive/MyDrive/ventilation-prediction-data/raw/radiology_test_notes.csv')
train_admissions_df = pd.read_csv('/content/drive/MyDrive/ventilation-prediction-data/raw/train_admissions.csv')
test_admissions_df = pd.read_csv('/content/drive/MyDrive/ventilation-prediction-data/raw/test_admissions.csv')
train_procedures_df = pd.read_csv('/content/drive/MyDrive/ventilation-prediction-data/raw/train_procedures.csv')
test_procedures_df = pd.read_csv('/content/drive/MyDrive/ventilation-prediction-data/raw/test_procedures.csv')
train_labevents_df = pd.read_csv('/content/drive/MyDrive/ventilation-prediction-data/raw/train_labevents.csv')
test_labevents_df = pd.read_csv('/content/drive/MyDrive/ventilation-prediction-data/raw/test_labevents.csv')
train_chartevents_df = pd.read_csv('/content/drive/MyDrive/ventilation-prediction-data/raw/train_chartevents.csv')
test_chartevents_df = pd.read_csv('/content/drive/MyDrive/ventilation-prediction-data/raw/test_chartevents.csv')
train_inputevents_df = pd.read_csv('/content/drive/MyDrive/ventilation-prediction-data/raw/train_inputevents.csv')
test_inputevents_df = pd.read_csv('/content/drive/MyDrive/ventilation-prediction-data/raw/test_inputevents.csv')
train_diagnoses_df = pd.read_csv('/content/drive/MyDrive/ventilation-prediction-data/raw/train_diagnoses.csv')
test_diagnoses_df = pd.read_csv('/content/drive/MyDrive/ventilation-prediction-data/raw/test_diagnoses.csv')
train_icustays_df = pd.read_csv('/content/drive/MyDrive/ventilation-prediction-data/raw/train_icustays.csv')
test_icustays_df = pd.read_csv('/content/drive/MyDrive/ventilation-prediction-data/raw/test_icustays.csv')

def create_six_hour_periods_df(admissions_df):
    '''
    Takes admission data and separates each hospital admission into disjoint six-hour periods.

    Parameters:
    - admissions_df: DataFrame with admission data (subject_id, hadm_id, admittime, dischtime)

    Returns:
    - DataFrame with six-hour periods for each admission
    '''
    # Convert admittime and dischtime to datetime format
    admissions_df['admittime'] = pd.to_datetime(admissions_df['admittime'])
    admissions_df['dischtime'] = pd.to_datetime(admissions_df['dischtime'])

    # Calculate total 6-hour periods per admission (ensure non-negative)
    total_periods = ((admissions_df['dischtime'] - admissions_df['admittime']).dt.total_seconds() // 21600).astype(int)
    total_periods = total_periods.clip(lower=0)  # Ensure no negative values

    # Expand DataFrame by repeating rows based on the number of 6-hour periods
    expanded_df = admissions_df.loc[admissions_df.index.repeat(total_periods)].copy()

    # Create a period index for each row (six-hour segment within admission)
    expanded_df['period_index'] = expanded_df.groupby(['subject_id', 'hadm_id']).cumcount()

    # Compute the start and end times for each six-hour period
    expanded_df['curr_period_start'] = expanded_df['admittime'] + pd.to_timedelta(expanded_df['period_index'] * 6, unit='h')
    expanded_df['curr_period_end'] = expanded_df['curr_period_start'] + pd.Timedelta(hours=6)

    # Select relevant columns and return as DataFrame
    return expanded_df[['subject_id', 'hadm_id', 'admittime', 'dischtime',
                        'curr_period_start', 'curr_period_end', 'period_index']]

# Apply the function to generate DataFrames
train_six_hour_periods = create_six_hour_periods_df(train_admissions_df)
test_six_hour_periods = create_six_hour_periods_df(test_admissions_df)

def remove_non_icu_periods(periods_df, icustays_df):
  '''
  Removes six-hour-periods that are not ICU stays.

  Parameters:
  - periods_df: DataFrame with six-hour periods
  - icustays_df: DataFrame with ICU stay data

  Returns:
  - DataFrame with non-ICU periods removed
  '''
  # Convert ICU times to datetime
  icustays_df = icustays_df.copy()
  icustays_df[['intime', 'outtime']] = icustays_df[['intime', 'outtime']].apply(pd.to_datetime)

  # Convert period time to datetime
  periods_df[['curr_period_start']] = periods_df[['curr_period_start']].apply(pd.to_datetime)

  # Merge the dfs on hadm_id
  merged_df = periods_df.merge(icustays_df, on=['hadm_id', 'subject_id'], how='left')

  mask = (merged_df['curr_period_start'] >= merged_df['intime']) & (merged_df['curr_period_start'] < merged_df['outtime'])

  # Only keep rows where the period is inside an ICU stay
  filtered_df = merged_df[mask]

  return filtered_df

train_six_hour_periods = remove_non_icu_periods(train_six_hour_periods, train_icustays_df)
test_six_hour_periods = remove_non_icu_periods(test_six_hour_periods, test_icustays_df)

def add_ventilation_indicator(new_data_df, chartevents_df, procedureevents_df):
    """
    Add ventilation indicator based on PEEP >= 5, FiO2 >= 21 within 5 minutes,
    and mechanical ventilation procedure (itemid = 225792) in the prediction window.

    Parameters:
    - new_data_df: DataFrame with 6-hour periods (hadm_id, curr_period_end, etc.)
    - chartevents_df: DataFrame with chartevents data (hadm_id, charttime, itemid, value)
    - procedureevents_df: DataFrame with procedureevents data (hadm_id, starttime, endtime, itemid)

    Returns:
    - new_data_df with ventilation_label column (1 if ventilated, 0 otherwise)
    """
    # Ensure correct data types
    chartevents_df['value'] = pd.to_numeric(chartevents_df['value'], errors='coerce')
    chartevents_df['charttime'] = pd.to_datetime(chartevents_df['charttime'])
    procedureevents_df['starttime'] = pd.to_datetime(procedureevents_df['starttime'])
    procedureevents_df['endtime'] = pd.to_datetime(procedureevents_df['endtime'])
    new_data_df['curr_period_end'] = pd.to_datetime(new_data_df['curr_period_end'])

    # Define prediction window times
    new_data_df['pred_window_start'] = new_data_df['curr_period_end'] + pd.Timedelta(hours=2)
    new_data_df['pred_window_end'] = new_data_df['curr_period_end'] + pd.Timedelta(hours=6)

    # Initialize ventilation labels
    new_data_df['ventilation_label'] = 0

    # Filter procedureevents for mechanical ventilation (itemid = 225792)
    vent_procedures = procedureevents_df[
        (procedureevents_df['itemid'] == 225792)
    ][['hadm_id', 'starttime', 'endtime']].copy()

    # Filter chartevents for PEEP >= 5 and FiO2 >= 21
    vent_peep_rows = chartevents_df[
        (chartevents_df['itemid'] == 220339) & (chartevents_df['value'] >= 5)
    ][['hadm_id', 'charttime', 'value']].copy()
    vent_fio2_rows = chartevents_df[
        (chartevents_df['itemid'] == 223835) & (chartevents_df['value'] >= 21)
    ][['hadm_id', 'charttime', 'value']].copy()

    # Merge PEEP and FiO2 on hadm_id and ensure within 5 minutes
    merged_peep_fio2 = pd.merge(
        vent_peep_rows,
        vent_fio2_rows,
        on='hadm_id',
        suffixes=('_peep', '_fio2')
    )
    merged_peep_fio2 = merged_peep_fio2[
        abs(merged_peep_fio2['charttime_peep'] - merged_peep_fio2['charttime_fio2']) <= pd.Timedelta(minutes=5)
    ][['hadm_id', 'charttime_peep', 'charttime_fio2']].copy()

    # Merge with procedureevents to ensure ventilation procedure is active
    vent_events = pd.merge(
        merged_peep_fio2,
        vent_procedures,
        on='hadm_id',
        how='inner'
    )

    # Filter events where PEEP/FiO2 and procedure overlap with prediction window
    vent_events = vent_events[
        # Ensure PEEP and FiO2 are within or after procedure starttime
        (vent_events['charttime_peep'] >= vent_events['starttime']) &
        (vent_events['charttime_fio2'] >= vent_events['starttime']) &
        # Ensure procedure is active during prediction window
        (vent_events['starttime'] <= vent_events['endtime']) &  # Procedure not ended prematurely
        (vent_events['endtime'].isna() | (vent_events['endtime'] >= vent_events['charttime_peep'])) &
        (vent_events['endtime'].isna() | (vent_events['endtime'] >= vent_events['charttime_fio2']))
    ]

    # Merge with new_data_df to assign labels
    new_data_df = new_data_df.merge(
        vent_events[['hadm_id', 'charttime_peep', 'charttime_fio2']],
        on='hadm_id',
        how='left'
    )

    # Assign ventilation label if PEEP/FiO2 fall within prediction window
    new_data_df['ventilation_label'] = (
        (new_data_df['charttime_peep'] >= new_data_df['pred_window_start']) &
        (new_data_df['charttime_peep'] <= new_data_df['pred_window_end']) &
        (new_data_df['charttime_fio2'] >= new_data_df['pred_window_start']) &
        (new_data_df['charttime_fio2'] <= new_data_df['pred_window_end'])
    ).astype(int)

    # Drop temporary columns
    new_data_df = new_data_df.drop(columns=['charttime_peep', 'charttime_fio2'], errors='ignore')

    return new_data_df

# Apply optimized function
train_six_hour_periods_labeled = add_ventilation_indicator(
    train_six_hour_periods, train_chartevents_df, train_procedures_df
)
test_six_hour_periods_labeled = add_ventilation_indicator(
    test_six_hour_periods, test_chartevents_df, test_procedures_df
)

def remove_periods_after_ventilation(df):
    """
    Removes all six-hour-periods after a patient was first put on mechanical
    ventilation in order to prevent data leakage.

    Parameters:
    - df: DataFrame with 6-hour periods

    Returns:
    - DataFrame with periods removed after ventilation
    """
    rows_to_remove = []  # Store indices of rows to remove

    # Iterate over unique hospital admission IDs
    for admission_id in df['hadm_id'].unique():
        # Filter rows for this hospital admission ID
        admission_periods = df[df['hadm_id'] == admission_id]

        # Find the first period the patient went on ventilation
        vent_periods = admission_periods[admission_periods['ventilation_label'] == 1]
        if not vent_periods.empty:
            # Find the first period of ventilation
            first_vent_period = vent_periods['period_index'].min()

            # Append rows after the first ventilation period to rows_to_remove
            rows_to_remove.extend(
                admission_periods[admission_periods['period_index'] > first_vent_period].index.tolist()
            )

    # Drop rows from the DataFrame
    df = df.drop(index=rows_to_remove)

    return df

train_six_hour_periods_labeled = remove_periods_after_ventilation(train_six_hour_periods_labeled)
test_six_hour_periods_labeled = remove_periods_after_ventilation(test_six_hour_periods_labeled)

# Downsample train set to get 5:1 ratio non-vent to vent for more class balance
vent_periods = train_six_hour_periods_labeled[train_six_hour_periods_labeled['ventilation_label'] == 1]
nonvent_periods = train_six_hour_periods_labeled[train_six_hour_periods_labeled['ventilation_label'] == 0]
downsampled_nonvent_df = nonvent_periods.sample(n=vent_periods.shape[0]*5, random_state=42)
downsampled_train_periods = pd.concat([vent_periods, downsampled_nonvent_df])

def add_relevant_labevents(new_data_df, labevents_df):
    """
    Add relevant lab events to new_data_df, storing valuenum and charttime for each itemid.

    Parameters:
    - new_data_df: DataFrame with 6-hour periods
    - labevents_df: DataFrame with lab events

    Returns:
    - new_data_df with relevant lab events
    """
    # Define relevant itemids for labevents (adjust based on your needs)
    relevant_lab_items = {50820, 50818, 50821, 50882, 50813, 50817, 50931, 50862, 51222, 50912}

    # Filter relevant labevents and convert to datetime
    labevents_df = labevents_df[labevents_df["itemid"].isin(relevant_lab_items)].copy()
    labevents_df['charttime'] = pd.to_datetime(labevents_df['charttime'])
    labevents_df = labevents_df.sort_values(['hadm_id', 'charttime'])

    # Prepare new_data_df
    new_data_df = new_data_df.copy()
    new_data_df['curr_period_start'] = pd.to_datetime(new_data_df['curr_period_start'])
    new_data_df['curr_period_end'] = pd.to_datetime(new_data_df['curr_period_end'])

    # Convert hadm_id to float (if necessary)
    new_data_df['hadm_id'] = new_data_df['hadm_id'].astype('float')
    labevents_df['hadm_id'] = labevents_df['hadm_id'].astype('float')

    # Merge using merge_asof
    merged_df = pd.merge_asof(
        labevents_df.sort_values('charttime'),
        new_data_df.sort_values('curr_period_start'),
        by='hadm_id',
        left_on='charttime',
        right_on='curr_period_start',
        direction='backward'
    )

    # Keep only events within the six-hour period
    merged_df = merged_df[
        (merged_df['charttime'] >= merged_df['curr_period_start']) &
        (merged_df['charttime'] < merged_df['curr_period_end'])
    ]

    # Group by hadm_id, curr_period_end, and itemid to collect valuenum and charttime
    grouped_events = (
        merged_df.groupby(['hadm_id', 'curr_period_end', 'itemid'])
        .agg({
            'valuenum': lambda x: x.tolist(),
            'charttime': lambda x: x.tolist()
        })
        .reset_index()
    )

    # Pivot to create columns for each itemid
    pivoted = grouped_events.pivot_table(
        index=['hadm_id', 'curr_period_end'],
        columns='itemid',
        values=['valuenum', 'charttime'],
        aggfunc='first'
    )

    # Flatten multi-index columns
    pivoted.columns = [f"{col[1]}_{col[0]}" for col in pivoted.columns]
    pivoted = pivoted.reset_index()

    # Ensure all relevant itemids have columns
    for itemid in relevant_lab_items:
        valuenum_col = f"{itemid}_valuenum"
        charttime_col = f"{itemid}_charttime"
        if valuenum_col not in pivoted.columns:
            pivoted[valuenum_col] = [[] for _ in range(len(pivoted))]
        if charttime_col not in pivoted.columns:
            pivoted[charttime_col] = [[] for _ in range(len(pivoted))]

    # Prevent duplicate columns before merging
    duplicate_cols = [col for col in pivoted.columns if col in new_data_df.columns and col not in ['hadm_id', 'curr_period_end']]
    if duplicate_cols:
        pivoted = pivoted.drop(columns=duplicate_cols)
        print(f"Dropped duplicate columns before merge: {duplicate_cols}")

    # Merge back into new_data_df
    new_data_df = new_data_df.merge(pivoted, on=['hadm_id', 'curr_period_end'], how='left')

    return new_data_df

downsampled_train_periods = add_relevant_labevents(downsampled_train_periods, train_labevents_df)
test_six_hour_periods_labeled = add_relevant_labevents(test_six_hour_periods_labeled, test_labevents_df)

def add_relevant_chartevents(new_data_df, chartevents_df):
    """
    Add relevant chart events (vitals) to new_data_df, storing valuenum and charttime for each itemid.

    Parameters:
    - new_data_df: DataFrame with 6-hour periods
    - chartevents_df: DataFrame with chartevents

    Returns:
    - new_data_df with relevant chartevents
    """
    # Define relevant itemids for chartevents (vitals)
    relevant_items = {220277, 220210, 220045, 220052, 223761, 223991, 224828,
                      226253, 223835, 223901, 223900, 220739}

    # Filter relevant chartevents and convert to datetime
    chartevents_df = chartevents_df[chartevents_df["itemid"].isin(relevant_items)].copy()
    chartevents_df['charttime'] = pd.to_datetime(chartevents_df['charttime'])
    chartevents_df = chartevents_df.sort_values(['hadm_id', 'charttime'])

    # Prepare new_data_df
    new_data_df = new_data_df.copy()
    new_data_df['curr_period_start'] = pd.to_datetime(new_data_df['curr_period_start'])
    new_data_df['curr_period_end'] = pd.to_datetime(new_data_df['curr_period_end'])

    # Convert hadm_id to float (if necessary)
    new_data_df['hadm_id'] = new_data_df['hadm_id'].astype('float')
    chartevents_df['hadm_id'] = chartevents_df['hadm_id'].astype('float')

    # Merge using merge_asof to associate chart events with periods
    merged_df = pd.merge_asof(
        chartevents_df.sort_values('charttime'),
        new_data_df.sort_values('curr_period_start'),
        by='hadm_id',
        left_on='charttime',
        right_on='curr_period_start',
        direction='backward'
    )

    # Keep only events within the six-hour period
    merged_df = merged_df[
        (merged_df['charttime'] >= merged_df['curr_period_start']) &
        (merged_df['charttime'] < merged_df['curr_period_end'])
    ]

    # Group by hadm_id, curr_period_end, and itemid to collect valuenum and charttime
    grouped_events = (
        merged_df.groupby(['hadm_id', 'curr_period_end', 'itemid'])
        .agg({
            'valuenum': lambda x: x.tolist(),  # Store as list
            'charttime': lambda x: x.tolist()  # Store as list
        })
        .reset_index()
    )

    # Pivot to create columns for each itemid
    pivoted = grouped_events.pivot_table(
        index=['hadm_id', 'curr_period_end'],
        columns='itemid',
        values=['valuenum', 'charttime'],
        aggfunc='first'  # Take the first (and only) list
    )

    # Flatten multi-index columns
    pivoted.columns = [f"{col[1]}_{col[0]}" for col in pivoted.columns]
    pivoted = pivoted.reset_index()

    # Ensure all relevant itemids have columns, filling missing with empty lists
    for itemid in relevant_items:
        valuenum_col = f"{itemid}_valuenum"
        charttime_col = f"{itemid}_charttime"
        if valuenum_col not in pivoted.columns:
            pivoted[valuenum_col] = [[] for _ in range(len(pivoted))]
        if charttime_col not in pivoted.columns:
            pivoted[charttime_col] = [[] for _ in range(len(pivoted))]

    # Prevent duplicate columns before merging
    duplicate_cols = [col for col in pivoted.columns if col in new_data_df.columns and col not in ['hadm_id', 'curr_period_end']]
    if duplicate_cols:
        pivoted = pivoted.drop(columns=duplicate_cols)
        print(f"Dropped duplicate columns before merge: {duplicate_cols}")

    # Merge back into new_data_df
    new_data_df = new_data_df.merge(pivoted, on=['hadm_id', 'curr_period_end'], how='left')

    return new_data_df

downsampled_train_periods = add_relevant_chartevents(downsampled_train_periods, train_chartevents_df)
test_six_hour_periods_labeled = add_relevant_chartevents(test_six_hour_periods_labeled, test_chartevents_df)

def add_relevant_diagnoses(new_data_df, diagnoses_df):
    """
    Add relevant diagnoses to new_data_df, storing ICD codes for each hadm_id.

    Parameters:
    - new_data_df: DataFrame with 6-hour periods
    - diagnoses_df: DataFrame with diagnoses

    Returns:
    - new_data_df with relevant diagnoses
    """
    # Define ICD categories with their corresponding ranges
    icd_categories = {
        "has_respiratory_disease": ["J00-J99"],
        "has_diabetes": ["E08-E13"],
        "has_hypertension": ["I10-I15"],
        "has_cancer": ["C00-C96"],
        "has_heart_failure": ["I50"],
        "has_renal_dysfunction": ["N17-N19"],
        "has_sepsis": ["A40-A41"],
        "has_neurological_disorder": ["G00-G99"],
        "has_obesity": ["E66"],
        "has_acute_cardiac_event": ["I21-I22"],
    }

    # Extract first letter and numeric part of the ICD codes
    diagnoses_df["icd_prefix"] = diagnoses_df["icd_code"].str[0]
    diagnoses_df["icd_numeric"] = diagnoses_df["icd_code"].str[1:].astype(str)

    # Create a mapping of hadm_id to diagnosis categories
    hadm_diagnosis_map = {}

    for category, ranges in icd_categories.items():
        for icd_range in ranges:
            prefix, range_str = icd_range[0], icd_range[1:]
            # Check if range_str contains a hyphen, if not, it's a single ICD code
            if "-" in range_str:
                # Split the range using the hyphen
                start_str, end_str = range_str.split("-")

                # Extract numeric part for range comparison
                range_start = int(start_str[1:]) if start_str[1:].isdigit() else -1
                range_end = int(end_str[1:]) if end_str[1:].isdigit() else -1

            else:
                # If it's a single ICD code, set range_start and range_end to the code's numeric part
                range_start = range_end = int(range_str[1:]) if range_str[1:].isdigit() else -1

            if range_start != -1:  # If range_start is valid, continue filtering
                # Filter out non-numeric values before converting to int
                numeric_icd_numeric = diagnoses_df["icd_numeric"][diagnoses_df["icd_numeric"].str.isdigit()]

                # Now apply astype(int) to the filtered Series
                matching_hadm_ids = diagnoses_df[
                    (diagnoses_df["icd_prefix"] == prefix) &
                    (diagnoses_df["icd_numeric"].isin(numeric_icd_numeric)) &  # Use isin for efficient filtering
                    (numeric_icd_numeric.astype(int).between(range_start, range_end)) # Convert to int safely
                ]["hadm_id"].unique()

                for hadm_id in matching_hadm_ids:
                    hadm_diagnosis_map.setdefault(hadm_id, set()).add(category)

    # Map diagnosis categories to each `hadm_id`
    new_data_df["diagnoses"] = new_data_df["hadm_id"].map(lambda x: hadm_diagnosis_map.get(x, set()))

    # Expand diagnosis flags into separate columns
    for category in icd_categories.keys():
        new_data_df[category] = new_data_df["diagnoses"].apply(lambda x: category in x)

    # Drop the temporary diagnoses column
    new_data_df.drop(columns=["diagnoses"], inplace=True)

    return new_data_df

downsampled_train_periods = add_relevant_diagnoses(downsampled_train_periods, train_diagnoses_df)
test_six_hour_periods_labeled = add_relevant_diagnoses(test_six_hour_periods_labeled, test_diagnoses_df)

def add_relevant_inputevents(new_data_df, inputevents_df):
    """
    Add relevant input events to new_data_df, storing valuenum and charttime for each itemid.

    Parameters:
    - new_data_df: DataFrame with 6-hour periods
    - inputevents_df: DataFrame with input events

    Returns:
    - new_data_df with relevant input events
    """
    # Define relevant itemids
    relevant_items = {225879, 225798, 225883, 225892}

    # Filter inputevents to only include relevant itemids
    filtered_inputevents = inputevents_df[inputevents_df["itemid"].isin(relevant_items)].copy()

    # Create a mapping of hadm_id to a set of relevant itemids
    hadm_input_map = (
        filtered_inputevents.groupby("hadm_id")["itemid"]
        .apply(set)
        .to_dict()
    )

    # Map input event presence to new_data
    for itemid in relevant_items:
        new_data_df[itemid] = new_data_df["hadm_id"].map(lambda x: itemid in hadm_input_map.get(x, set()))

    return new_data_df

downsampled_train_periods = add_relevant_inputevents(downsampled_train_periods, train_inputevents_df)
test_six_hour_periods_labeled = add_relevant_inputevents(test_six_hour_periods_labeled, test_inputevents_df)

def add_relevant_demographics(new_data_df, patients_df):
    """
    Add relevant demographics to new_data_df.

    Parameters:
    - new_data_df: DataFrame with 6-hour periods
    - patients_df: DataFrame with patients

    Returns:
    - new_data_df with relevant demographics
    """
    # Merge with patients_df to bring in demographics
    new_data_df = new_data_df.merge(
        patients_df[['subject_id', 'anchor_age', 'gender']],
        on='subject_id',
        how='left'
    )

    # Rename columns to match expected keys
    new_data_df.rename(columns={'anchor_age': 'Age', 'gender': 'Gender'}, inplace=True)

    return new_data_df

downsampled_train_periods = add_relevant_demographics(downsampled_train_periods, train_patients_df)
test_six_hour_periods_labeled = add_relevant_demographics(test_six_hour_periods_labeled, test_patients_df)

def add_relevant_notes(new_data_df, notes_df):
    """
    Add relevant radiology reports within the six-hour-period to new_data_df.

    Parameters:
    - new_data_df: DataFrame with 6-hour periods
    - notes_df: DataFrame with radiology report notes.

    Returns:
    - new_data_df with relevant notes
    """
    # Convert storetime to datetime
    notes_df['storetime'] = pd.to_datetime(notes_df['storetime'])
    new_data_df['curr_period_start'] = pd.to_datetime(new_data_df['curr_period_start'])
    new_data_df['curr_period_end'] = pd.to_datetime(new_data_df['curr_period_end'])

    # Filter notes within the min-max range **before merging** to reduce data size
    filtered_notes = notes_df[
        (notes_df['storetime'] >= new_data_df['curr_period_start'].min()) &
        (notes_df['storetime'] < new_data_df['curr_period_end'].max())
    ]

    # Merge to associate `hadm_id` and filter relevant time range
    merged_df = new_data_df.merge(filtered_notes, on='hadm_id', how='left')

    # Keep only notes within the period's time window
    merged_df = merged_df[
        (merged_df['storetime'] >= merged_df['curr_period_start']) &
        (merged_df['storetime'] < merged_df['curr_period_end'])
    ]

    # Group by `hadm_id` and `curr_period_end` and concatenate text
    grouped_notes = merged_df.groupby(['hadm_id', 'curr_period_end'])['text'].agg(' '.join).reset_index()

    # Merge concatenated notes back into new_data_df
    new_data_df = new_data_df.merge(grouped_notes, on=['hadm_id', 'curr_period_end'], how='left')

    # Fill missing notes with empty strings
    new_data_df['text'] = new_data_df['text'].fillna('')

    # Rename column to match expected output
    new_data_df.rename(columns={'text': 'Notes'}, inplace=True)

    return new_data_df

downsampled_train_periods = add_relevant_notes(downsampled_train_periods, train_notes_df)
test_six_hour_periods_labeled = add_relevant_notes(test_six_hour_periods_labeled, test_notes_df)

def create_relative_hourly_bins(df):
    """
    Create hourly bins (0-5) for each itemid, computing mean valuenum per hour.
    This is for use in the lstm model.

    Parameters:
    - df: DataFrame with 6-hour periods

    Returns:
    - df with hourly bins for each itemid
    """
    df_copy = df.copy()

    # Identify columns containing valuenum and charttime
    valuenum_cols = [col for col in df_copy.columns if '_valuenum' in str(col)]
    itemids = {int(col.split('_')[0]) for col in valuenum_cols}

    # Process each itemid
    for itemid in itemids:
        valuenum_col = f"{itemid}_valuenum"
        charttime_col = f"{itemid}_charttime"
        # Skip if columns don't exist
        if valuenum_col not in df_copy.columns or charttime_col not in df_copy.columns:
            continue

        # Create a long-form DataFrame for this itemid
        long_df = []
        for idx, row in df_copy.iterrows():
            valuenum_list = row[valuenum_col] if isinstance(row[valuenum_col], list) else []
            charttime_list = row[charttime_col] if isinstance(row[charttime_col], list) else []
            if len(valuenum_list) != len(charttime_list):
                continue  # Skip malformed rows
            for val, time in zip(valuenum_list, charttime_list):
                if pd.notna(val) and pd.notna(time):
                    long_df.append({
                        'index': idx,
                        'hadm_id': row['hadm_id'],
                        'curr_period_end': row['curr_period_end'],
                        'curr_period_start': row['curr_period_start'],
                        'valuenum': val,
                        'charttime': pd.to_datetime(time)
                    })

        if not long_df:
            # If no data, create empty bin columns
            for hour in range(6):
                df_copy[f"{itemid}_{hour}"] = [None] * len(df_copy)
            continue

        long_df = pd.DataFrame(long_df)

        # Compute relative hour (0-5)
        long_df['relative_hour'] = (
            (long_df['charttime'] - long_df['curr_period_start']).dt.total_seconds() // 3600
        ).astype('Int64')

        # Group by original index and relative_hour to compute mean valuenum
        hourly_means = (
            long_df.groupby(['index', 'relative_hour'])['valuenum']
            .mean()
            .reset_index()
        )

        # Pivot to create 6 bins (0-5)
        pivoted = hourly_means.pivot_table(
            index='index',
            columns='relative_hour',
            values='valuenum',
            fill_value=None
        )

        # Reindex to ensure all 6 hours are present
        pivoted = pivoted.reindex(columns=range(6), fill_value=None)

        # Convert to list of 6 bins and assign to new columns
        bin_lists = pivoted.apply(lambda x: x.tolist(), axis=1)
        for hour in range(6):
            col_name = f"{itemid}_{hour}"
            df_copy[col_name] = bin_lists.apply(lambda x: x[hour] if isinstance(x, list) else None)

    return df_copy

train_six_hour_periods_labeled_binned = create_relative_hourly_bins(downsampled_train_periods)
test_six_hour_periods_labeled_binned = create_relative_hourly_bins(test_six_hour_periods_labeled)

def aggregate_data(df):
    """
    Aggregate measurements for each itemid by computing mean, min, max, and slope.
    Uses the _valuenum columns containing lists of measurements.

    Parameters:
    - df: DataFrame with 6-hour periods

    Returns:
    - df with aggregated measurements for each itemid
    """
    new_df = df.copy()

    # Identify columns containing valuenum measurements
    relevant_cols = [col for col in new_df.columns if '_valuenum' in col]
    itemids = {int(col.split('_')[0]) for col in relevant_cols}

    for itemid in itemids:
        valuenum_col = f"{itemid}_valuenum"
        charttime_col = f"{itemid}_charttime"

        if valuenum_col not in new_df.columns:
            continue  # Skip if valuenum column doesn't exist

        # Compute mean, min, max
        new_df[f'Mean {itemid}'] = new_df[valuenum_col].apply(
            lambda x: np.mean(x) if isinstance(x, list) and x and all(pd.notna(y) for y in x) else None
        )
        new_df[f'Min {itemid}'] = new_df[valuenum_col].apply(
            lambda x: np.min(x) if isinstance(x, list) and x and all(pd.notna(y) for y in x) else None
        )
        new_df[f'Max {itemid}'] = new_df[valuenum_col].apply(
            lambda x: np.max(x) if isinstance(x, list) and x and all(pd.notna(y) for y in x) else None
        )

        # Compute slope (using charttime for time points)
        if charttime_col in new_df.columns:
            def compute_slope(row):
                valuenum_list = row[valuenum_col] if isinstance(row[valuenum_col], list) else []
                charttime_list = row[charttime_col] if isinstance(row[charttime_col], list) else []
                if len(valuenum_list) != len(charttime_list) or len(valuenum_list) < 2:
                    return None  # Need at least 2 points for slope
                try:
                    # Convert charttime to seconds since curr_period_start for numerical x-axis
                    charttime = [pd.to_datetime(t) for t in charttime_list]
                    start_time = row['curr_period_start']
                    time_diffs = [(t - start_time).total_seconds() / 3600 for t in charttime]  # Hours
                    if not all(pd.notna(v) for v in valuenum_list) or not all(pd.notna(t) for t in time_diffs):
                        return None
                    slope, _, _, _, _ = linregress(time_diffs, valuenum_list)
                    return slope
                except Exception:
                    return None
            new_df[f'Slope {itemid}'] = new_df.apply(compute_slope, axis=1)
        else:
            new_df[f'Slope {itemid}'] = [None] * len(new_df)

    return new_df

train_aggregated_six_hour_periods = aggregate_data(downsampled_train_periods)
test_aggregated_six_hour_periods = aggregate_data(test_six_hour_periods_labeled)

def rename_columns(new_data):
  # Rename columns with ids to their labels
  id_to_label = {220277: 'O2_saturation', 220210: 'Respiratory_rate', 220045: 'Heart_rate', 220052: 'Arterial Blood Pressure',
                                    223761: 'Temperature', 223991: 'Cough Effort', 50820: 'pH', 50818: 'pCO2', 50817: 'Oxygen Saturation',
                                    50821: 'pO2', 50882: 'Bicarbonate', 50813: 'Lactate', 50931: 'Glucose', 225879: 'Levofloxacin',
                                    225798: 'Vancomycin', 225883: 'Meropenem', 225892: 'Piperacillin', 50912: 'Creatinine', 51222: 'Hemoglobin',
                                    50862: 'Albumin', 224828: 'Arterial base excess'}

  # Convert new data to a df
  full_df = pd.DataFrame(new_data)

  # Function to rename columns based on id_to_label mapping
  def rename_column(col_name):
    col_name = str(col_name)
    for id_num, label in id_to_label.items():
      id_str = str(id_num)
      if id_str in col_name:
        # Replace the numeric part with the descriptive label + numeric ID
        return col_name.replace(id_str, f"{label}")
    return col_name  # Return original name if no match found

  # Rename columns dynamically
  full_df = full_df.rename(columns=rename_column)

  # Drop columns containing specific strings
  columns_to_drop = ['226253', '223835', '223901', '223900', '220739']
  full_df = full_df.drop(columns=[col for col in full_df.columns if any(drop_str in col for drop_str in columns_to_drop)])

  # Fill down the ventilation column with False if not True
  if 'on_ventilation' in full_df.columns:
    full_df['on_ventilation'] = full_df['on_ventilation'].fillna(False)

  return full_df

# Apply rename columns function to aggregated data
train_converted_df_aggregated = rename_columns(train_aggregated_six_hour_periods)
test_converted_df_aggregated = rename_columns(test_aggregated_six_hour_periods)

# Apply rename columns function to time series data
train_time_series_df = rename_columns(train_six_hour_periods_labeled_binned)
test_time_series_df = rename_columns(test_six_hour_periods_labeled_binned)

# Warning about DUA
print("Warning: The following dataframes are derived from MIMIC-IV and should not be shared publicly. "
      "Ensure compliance with the MIMIC-IV Data Use Agreement.")
print("Note: Clinical notes in train_time_series_vent_data.csv and train_wrangled_df.csv are sensitive. "
      "Store securely and do not share.")

# Save data
os.makedirs(args.output_dir, exist_ok=True)
train_time_series_df.to_csv(os.path.join(args.output_dir, 'train_time_series_vent_data.csv'), index=False)
test_time_series_df.to_csv(os.path.join(args.output_dir, 'test_time_series_vent_data.csv'), index=False)
train_converted_df_aggregated.to_csv(os.path.join(args.output_dir, 'train_wrangled_df.csv'), index=False)
test_converted_df_aggregated.to_csv(os.path.join(args.output_dir, 'test_wrangled_df.csv'), index=False)

