# -*- coding: utf-8 -*-
"""ventilation_feature_selection_and_baseline_modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GLmXyNmPx8S3qHX2dK6GUoGpd5Gnk9t0

# Ventilation Prediction: Feature Selection and Modeling

This notebook further processes data from `ventilation_wrangling.py` to predict ventilation. It:
- Performs feature selection using Boruta (random forest wrapper)
- Establishes a baseline performance using logistic regression
- Applies error analysis the logistic regression
- Trains and evaluates random forest and XGBoost models with 5-fold cross-validation

Install dependencies with: `pip install -r requirements.txt`
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb
import seaborn as sns
import argparse
import os
from tqdm.notebook import tqdm
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report
from boruta import BorutaPy
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import (
    balanced_accuracy_score,
    precision_recall_curve,
    auc,
    f1_score,
    classification_report,
    confusion_matrix,
    precision_recall_fscore_support
)
from imblearn.pipeline import Pipeline as ImbPipeline

"""# Boruta feature selection"""

def parse_args():
    parser = argparse.ArgumentParser(description="Feature selection and modeling for ventilation prediction.")
    parser.add_argument('--input_dir', type=str, default='data/wrangled/',
                       help='Directory containing wrangled MIMIC-IV CSV files.')
    parser.add_argument('--output_dir', type=str, default='data/models/',
                       help='Directory to save model outputs or results.')
    return parser.parse_args()

# For notebook, set manually (replace with parse_args() in .py)
args = type('Args', (), {
    'input_dir': 'data/wrangled/',
    'output_dir': 'data/models/'
})()

# Load data
print("Warning: The input CSVs (train_wrangled_df.csv, test_wrangled_df.csv) contain MIMIC-IV-derived data, "
      "including sensitive clinical notes (72.6% empty). Do not share publicly. Ensure compliance with the MIMIC-IV DUA.")
try:
    train_df = pd.read_csv(os.path.join(args.input_dir, 'train_wrangled_df.csv'))
    test_df = pd.read_csv(os.path.join(args.input_dir, 'test_wrangled_df.csv'))
except FileNotFoundError as e:
    print(f"Error: CSV file not found in {args.input_dir}: {e}")
    raise

# Split test set into validation and test set at 1:1 ratio, stratifying by ventilation
val_df, test_df = train_test_split(test_df, train_size = 0.5, random_state = 42, stratify = test_df['ventilation_label'])

# Drop irrelevant columns for prediction
train_df = train_df.drop(['Unnamed: 0', 'Unnamed: 0.1', 'curr_period_start', 'curr_period_end', 'period_index', 'stay_id', 'subject_id', 'hadm_id', 'admittime', 'dischtime', 'curr_period_start', 'curr_period_end', 'period_index', 'first_careunit', 'last_careunit', 'intime', 'outtime', 'pred_window_start', 'pred_window_end'], axis=1)
val_df = val_df.drop(['Unnamed: 0', 'Unnamed: 0.1', 'curr_period_start', 'curr_period_end', 'period_index', 'stay_id', 'subject_id', 'hadm_id', 'admittime', 'dischtime', 'curr_period_start', 'curr_period_end', 'period_index', 'first_careunit', 'last_careunit', 'intime', 'outtime', 'pred_window_start', 'pred_window_end'], axis=1)

# Drop any feature that contains 'charttime' or 'valuenum'
train_df = train_df.drop([col for col in train_df.columns if 'charttime' in col or 'valuenum' in col], axis=1)
val_df = val_df.drop([col for col in val_df.columns if 'charttime' in col or 'valuenum' in col], axis=1)

# For static variables, treat missing as false
train_df['has_respiratory_disease'] = train_df['has_respiratory_disease'].fillna(False)
train_df['has_hypertension'] = train_df['has_hypertension'].fillna(False)
train_df['has_neurological_disorder'] = train_df['has_neurological_disorder'].fillna(False)
train_df['has_cancer'] = train_df['has_cancer'].fillna(False)
train_df['has_heart_failure'] = train_df['has_heart_failure'].fillna(False)
train_df['has_diabetes'] = train_df['has_diabetes'].fillna(False)
train_df['has_renal_dysfunction'] = train_df['has_renal_dysfunction'].fillna(False)
train_df['Piperacillin'] = train_df['Piperacillin'].fillna(False)
train_df['Levofloxacin'] = train_df['Levofloxacin'].fillna(False)
train_df['Meropenem'] = train_df['Meropenem'].fillna(False)
train_df['Vancomycin'] = train_df['Vancomycin'].fillna(False)
val_df['has_respiratory_disease'] = val_df['has_respiratory_disease'].fillna(False)
val_df['has_hypertension'] = val_df['has_hypertension'].fillna(False)
val_df['has_neurological_disorder'] = val_df['has_neurological_disorder'].fillna(False)
val_df['has_cancer'] = val_df['has_cancer'].fillna(False)
val_df['has_heart_failure'] = val_df['has_heart_failure'].fillna(False)
val_df['has_diabetes'] = val_df['has_diabetes'].fillna(False)
val_df['has_renal_dysfunction'] = val_df['has_renal_dysfunction'].fillna(False)
val_df['Piperacillin'] = val_df['Piperacillin'].fillna(False)
val_df['Levofloxacin'] = val_df['Levofloxacin'].fillna(False)
val_df['Meropenem'] = val_df['Meropenem'].fillna(False)
val_df['Vancomycin'] = val_df['Vancomycin'].fillna(False)

# Drop features with less than 10% nonmissing values
percent_nonmissing = train_df.count().sort_values(ascending=False).apply(lambda x: x/len(train_df))
features_to_drop = percent_nonmissing[percent_nonmissing < 0.1].index
train_df = train_df.drop(percent_nonmissing[percent_nonmissing < 0.1].index, axis=1)
val_df = val_df.drop(features_to_drop, axis=1)

# Remove bool vars with < 1% positive values
boolean_cols = train_df.columns[train_df.dtypes == 'bool']
for col in boolean_cols:
  if train_df[col].value_counts(normalize=True).min() < 0.01:
    print(col)
    train_df = train_df.drop(col, axis=1)
    val_df = val_df.drop(col, axis=1)

# Convert nonnumeric cols to binary
train_df['Gender'] = train_df['Gender'].apply(lambda x: 1 if x == 'M' else 0)
val_df['Gender'] = val_df['Gender'].apply(lambda x: 1 if x == 'M' else 0)

# Separate notes to be rejoined later
train_notes = train_df['Notes']
val_notes = val_df['Notes']

train_df = train_df.drop('Notes', axis=1)
val_df = val_df.drop('Notes', axis=1)

# Add missingness indicators
def add_missingness_indicators(df, target_col):
  # Find features with missing values
  features_with_missing = df.columns[df.isnull().any()].tolist()
  missingness_indicators = {}

  # Create missingness indicators
  for feature in features_with_missing:
    df[f'{feature}_missing'] = df[feature].isnull().astype(int)
    missingness_indicators[f'{feature}_missing'] = 1

  # Impute missing values for original features
  imputer = SimpleImputer(strategy='mean')
  df[df.columns.difference([target_col])] = imputer.fit_transform(df[df.columns.difference([target_col])])

  return df, missingness_indicators

train_prepped, train_missingness_indicators = add_missingness_indicators(train_df, target_col='ventilation_label')
val_df_prepped, val_missingness_indicators = add_missingness_indicators(val_df, target_col='ventilation_label')

# Split up predictors and target variables
X_train = train_prepped.drop('ventilation_label', axis=1)
y_train = train_prepped['ventilation_label']
X_val = val_df_prepped.drop('ventilation_label', axis=1)
y_val = val_df_prepped['ventilation_label']

"""## Boruta Feature Selection
Uses Boruta to select features from 121 candidates, comparing importance to shadow features (`perc=75`, `n_estimators=50`).
"""

# Initialize rf classifier
rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5, random_state=42)

# Initialize Boruta with the rf model
# Perc is a percentile of shadow feature importance scores as the threshold
boruta = BorutaPy(rf, n_estimators=50, perc=75, random_state=42)

# Fit boruta on data
for _ in tqdm(range(boruta.n_estimators), desc="Boruta Feature Selection"):
  boruta.fit(X_train.values, y_train.values)

# Get selected features
selected_features = X_train.columns[boruta.support_].tolist()
weak_features = X_train.columns[boruta.support_weak_].tolist()
excluded_features = [feature for feature in X_train.columns if feature not in selected_features]

print('Selected features:', selected_features)
print('Weakly selected features:', weak_features)
print('Excluded features:', excluded_features)

len(X_train.columns), len(excluded_features), len(selected_features)

# Only keep selected features
X_val_boruta = X_val[selected_features]
X_train_boruta = X_train[selected_features]

# Compute the correlation matrix
corr_matrix = X_train_boruta.corr().abs()

# Identify features with correlation = 1 (excluding self-correlation)
upper_triangle = corr_matrix.where(
    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
)
perfect_corr_pairs = [
    (column, index) for column in upper_triangle.columns
    for index in upper_triangle.index
    if upper_triangle.at[index, column] == 1
]

# Drop one feature from each pair
features_to_drop = set([pair[1] for pair in perfect_corr_pairs])
print(f"Features to drop: {features_to_drop}")
X_train_boruta_reduced = X_train_boruta.drop(columns=features_to_drop)

# Drop the same features from the validaiton set
X_val_boruta_reduced = X_val_boruta[X_train_boruta_reduced.columns]

# Only keep 1 missing indicator of each
X_train_boruta_reduced = X_train_boruta_reduced.drop(columns=['Slope pCO2_missing', 'Slope pCO2_missing', 'Slope pH_missing', 'Slope pO2_missing', 'Slope Arterial Blood Pressure_missing', 'Slope Arterial base excess_missing'])
X_val_boruta_reduced = X_val_boruta_reduced.drop(columns=['Slope pCO2_missing', 'Slope pCO2_missing', 'Slope pH_missing', 'Slope pO2_missing', 'Slope Arterial Blood Pressure_missing', 'Slope Arterial base excess_missing'])

# Drop los
X_train_boruta_reduced = X_train_boruta_reduced.drop(columns=['los'])
X_val_boruta_reduced = X_val_boruta_reduced.drop(columns=['los'])

# Make sure indices of notes df and X_val_boruta_reduced are aligned
val_notes = val_notes.reset_index(drop=True)
train_notes = train_notes.reset_index(drop=True)
X_train_boruta_reduced = X_train_boruta_reduced.reset_index(drop=True)
X_val_boruta_reduced = X_val_boruta_reduced.reset_index(drop=True)
X_train_boruta_reduced = X_train_boruta_reduced.reset_index(drop=True)

# Add notes back
X_val_boruta_reduced = pd.concat([X_val_boruta_reduced, val_notes], axis=1)
X_train_boruta_reduced = pd.concat([X_train_boruta_reduced, train_notes], axis=1)

# Impute missing values in the 'Notes' column with an empty string
X_val_boruta_reduced['Notes'] = X_val_boruta_reduced['Notes'].fillna('')

def process_and_evaluate_model(X, y, X_val=None, y_val=None, recall_weight=2.0):
    """
    Performs 5-fold cross-validation on the training set and evaluates on a validation set with optimized threshold.

    Parameters:
    - X: Training features (train set, e.g., 5:1 class ratio).
    - y: Training labels.
    - X_val: Validation features (true class distribution).
    - y_val: Validation labels.
    - recall_weight: Weight for recall in custom F1-score (default 2.0 to prioritize recall).

    Returns:
    - pipeline: Trained model pipeline.
    - balanced_acc_cv: Balanced accuracy from cross-validation.
    - pr_auc_cv: Precision-Recall AUC from cross-validation.
    - f1_cv: F1-score from cross-validation.
    - threshold: Optimal threshold for validation set predictions.
    """
    # Split numerical and text columns
    numerical_columns = X.drop(columns=['Notes']).columns.tolist()
    text_column = 'Notes'

    # Impute missing values in the 'Notes' column with an empty string
    X = X.copy()
    X['Notes'] = X['Notes'].fillna('')
    if X_val is not None:
        X_val = X_val.copy()
        X_val['Notes'] = X_val['Notes'].fillna('')

    # Define transformers
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean')),  # Impute missing numerical values
        ('scaler', StandardScaler())  # Standardize numerical features
    ])
    text_transformer = Pipeline(steps=[
        ('tfidf', TfidfVectorizer(max_features=500))  # Limit to 500 text features
    ])

    # Combine transformers
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numerical_columns),
            ('text', text_transformer, text_column)
        ]
    )

    # Define class weights and pipeline
    class_weight = {0: 1, 1: 10}
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', LogisticRegression(
            max_iter=7000, class_weight=class_weight, solver='liblinear', random_state=42
        ))
    ])

    # 5-Fold Cross-Validation on Training Set
    print("Performing 5-fold cross-validation on training set...")
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    y_pred_cv = cross_val_predict(pipeline, X, y, cv=skf, method='predict')
    y_pred_prob_cv = cross_val_predict(pipeline, X, y, cv=skf, method='predict_proba')[:, 1]

    # Calculate metrics for cross-validation
    balanced_acc_cv = balanced_accuracy_score(y, y_pred_cv)
    precision_cv, recall_cv, _ = precision_recall_curve(y, y_pred_prob_cv)
    pr_auc_cv = auc(recall_cv, precision_cv)
    f1_cv = f1_score(y, y_pred_cv)

    print("\nCross-Validation Metrics (Training Set, 5:1 ratio):")
    print(f"Balanced Accuracy: {balanced_acc_cv:.4f}")
    print(f"PR AUC: {pr_auc_cv:.4f}")
    print(f"F1-Score: {f1_cv:.4f}")
    print("\nClassification Report (Cross-Validation):")
    print(classification_report(y, y_pred_cv))

    # Evaluate on Validation Set (true class distribution)
    threshold = 0.5  # Default threshold
    if X_val is not None and y_val is not None:
        print("\nEvaluating on the validation set (true class distribution)...")
        pipeline.fit(X, y)  # Train on the entire training set
        y_val_pred_prob = pipeline.predict_proba(X_val)[:, 1]

        # Optimize threshold based on weighted F1-score
        precision, recall, thresholds = precision_recall_curve(y_val, y_val_pred_prob)
        f1_scores = []
        for t in thresholds:
            y_pred = (y_val_pred_prob >= t).astype(int)
            p, r, _, _ = precision_recall_fscore_support(y_val, y_pred, average='binary', zero_division=0)
            # Weighted F1: (1 + beta^2) * (p * r) / (beta^2 * p + r), where beta = recall_weight
            beta = recall_weight
            f1_weighted = (1 + beta**2) * (p * r) / (beta**2 * p + r + 1e-10) if (beta**2 * p + r) > 0 else 0
            f1_scores.append(f1_weighted)

        # Select threshold maximizing weighted F1-score
        optimal_idx = np.argmax(f1_scores)
        threshold = thresholds[optimal_idx]
        y_val_pred = (y_val_pred_prob >= threshold).astype(int)

        # Calculate metrics for validation set
        balanced_acc_val = balanced_accuracy_score(y_val, y_val_pred)
        precision_val, recall_val, _ = precision_recall_curve(y_val, y_val_pred_prob)
        pr_auc_val = auc(recall_val, precision_val)
        f1_val = f1_score(y_val, y_val_pred)

        print(f"\nOptimal Threshold: {threshold:.4f}")
        print("\nValidation Set Metrics (True Distribution):")
        print(classification_report(y_val, y_val_pred))
        print("\nConfusion Matrix (Validation Set):")
        print(confusion_matrix(y_val, y_val_pred))
        print(f"Balanced Accuracy: {balanced_acc_val:.4f}")
        print(f"PR AUC: {pr_auc_val:.4f}")
        print(f"F1-Score: {f1_val:.4f}")

    return pipeline, balanced_acc_cv, pr_auc_cv, f1_cv, threshold

notes_evaluated_pipeline, notes_balanced_acc_cv, notes_pr_auc_cv, notes_f1_cv, notes_threshold = process_and_evaluate_model(
    X_train_boruta_reduced, y_train, X_val_boruta_reduced, y_val, recall_weight=5.0
)

def process_and_evaluate_model_no_notes(X, y, X_val=None, y_val=None, recall_weight=2.0):
    """
    Performs 5-fold cross-validation on the training set and evaluates on a validation set with optimized threshold.
    Excluding the 'Notes' column from the features.

    Parameters:
    - X: Training features (train set, e.g., 5:1 class ratio).
    - y: Training labels.
    - X_val: Validation features (true class distribution).
    - y_val: Validation labels.
    - recall_weight: Weight for recall in custom F1-score (default 2.0 to prioritize recall).

    Returns:
    - pipeline: Trained model pipeline.
    - balanced_acc_cv: Balanced accuracy from cross-validation.
    - pr_auc_cv: Precision-Recall AUC from cross-validation.
    - f1_cv: F1-score from cross-validation.
    - threshold: Optimal threshold for validation set predictions.
    """

    # Define transformers
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean')),  # Impute missing numerical values
        ('scaler', StandardScaler())  # Standardize numerical features
    ])

    # Combine transformers
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, X),
        ]
    )

    # Define class weights and pipeline
    class_weight = {0: 1, 1: 10}
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', LogisticRegression(
            max_iter=7000, class_weight=class_weight, solver='liblinear', random_state=42
        ))
    ])

    # 5-Fold Cross-Validation on Training Set
    print("Performing 5-fold cross-validation on training set...")
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    y_pred_cv = cross_val_predict(pipeline, X, y, cv=skf, method='predict')
    y_pred_prob_cv = cross_val_predict(pipeline, X, y, cv=skf, method='predict_proba')[:, 1]

    # Calculate metrics for cross-validation
    balanced_acc_cv = balanced_accuracy_score(y, y_pred_cv)
    precision_cv, recall_cv, _ = precision_recall_curve(y, y_pred_prob_cv)
    pr_auc_cv = auc(recall_cv, precision_cv)
    f1_cv = f1_score(y, y_pred_cv)

    print("\nCross-Validation Metrics (Training Set, 5:1 ratio):")
    print(f"Balanced Accuracy: {balanced_acc_cv:.4f}")
    print(f"PR AUC: {pr_auc_cv:.4f}")
    print(f"F1-Score: {f1_cv:.4f}")
    print("\nClassification Report (Cross-Validation):")
    print(classification_report(y, y_pred_cv))

    # Evaluate on Validation Set (true class distribution)
    threshold = 0.5  # Default threshold
    if X_val is not None and y_val is not None:
        print("\nEvaluating on the validation set (true class distribution)...")
        pipeline.fit(X, y)  # Train on the entire training set
        y_val_pred_prob = pipeline.predict_proba(X_val)[:, 1]

        # Optimize threshold based on weighted F1-score
        precision, recall, thresholds = precision_recall_curve(y_val, y_val_pred_prob)
        f1_scores = []
        for t in thresholds:
            y_pred = (y_val_pred_prob >= t).astype(int)
            p, r, _, _ = precision_recall_fscore_support(y_val, y_pred, average='binary', zero_division=0)
            # Weighted F1: (1 + beta^2) * (p * r) / (beta^2 * p + r), where beta = recall_weight
            beta = recall_weight
            f1_weighted = (1 + beta**2) * (p * r) / (beta**2 * p + r + 1e-10) if (beta**2 * p + r) > 0 else 0
            f1_scores.append(f1_weighted)

        # Select threshold maximizing weighted F1-score
        optimal_idx = np.argmax(f1_scores)
        threshold = thresholds[optimal_idx]
        y_val_pred = (y_val_pred_prob >= threshold).astype(int)

        # Calculate metrics for validation set
        balanced_acc_val = balanced_accuracy_score(y_val, y_val_pred)
        precision_val, recall_val, _ = precision_recall_curve(y_val, y_val_pred_prob)
        pr_auc_val = auc(recall_val, precision_val)
        f1_val = f1_score(y_val, y_val_pred)

        print(f"\nOptimal Threshold: {threshold:.4f}")
        print("\nValidation Set Metrics (True Distribution):")
        print(classification_report(y_val, y_val_pred))
        print("\nConfusion Matrix (Validation Set):")
        print(confusion_matrix(y_val, y_val_pred))
        print(f"Balanced Accuracy: {balanced_acc_val:.4f}")
        print(f"PR AUC: {pr_auc_val:.4f}")
        print(f"F1-Score: {f1_val:.4f}")

    return pipeline, balanced_acc_cv, pr_auc_cv, f1_cv, threshold

X_val_boruta_reduced_no_notes = X_val_boruta_reduced.drop(columns=['Notes'])
X_train_boruta_reduced_no_notes = X_train_boruta_reduced.drop(columns=['Notes'])
evaluated_pipeline, balanced_acc_cv, pr_auc_cv, f1_cv, threshold = process_and_evaluate_model(
    X_train_boruta_reduced, y_train, X_val_boruta_reduced, y_val, recall_weight=1.0
)

y_pred = notes_evaluated_pipeline.predict(X_val_boruta_reduced)

def calculate_error_analysis(X_test, y_test, y_pred, model):
    """
    Calculates horizontal absolute differences and feature weights for logistic regression.
    Identifies features that might influence false positives.

    Parameters:
    - X_test: Test set features (pandas DataFrame).
    - y_test: True labels for the test set (numpy array or pandas Series).
    - y_pred: Predicted labels from the model (numpy array).
    - model: Trained logistic regression model (pipeline or standalone model).

    Returns:
    - results: DataFrame with feature weights and HAD for false positives.
    """

    y_test = y_test.reset_index(drop=True)

    # Identify false positives and true negatives
    false_positives = (y_test == 0) & (y_pred == 1)
    true_negatives = (y_test == 0) & (y_pred == 0)

    # **Exclude the 'Notes' column before calculating the mean**
    numerical_features = X_test.select_dtypes(include=np.number).columns
    X_test_numerical = X_test[numerical_features]

    # Extract feature coefficients (logistic regression weights)
    classifier = model.named_steps['classifier']  # Adjust if pipeline
    feature_weights = classifier.coef_[0]  # Coefficients for each feature

    # Compute mean feature values for true negatives (using only numerical features)
    mean_tn_values = X_test_numerical[true_negatives].mean()

    # Compute horizontal absolute differences for false positives (using only numerical features)
    had_fp = X_test_numerical[false_positives].apply(lambda row: np.abs(row - mean_tn_values), axis=1)

    # Average horizontal absolute differences for false positives
    mean_had_fp = had_fp.mean()

    # Create a results DataFrame (using only numerical features)
    results = pd.DataFrame({
        'Feature': numerical_features,
        'Feature Weight': feature_weights[:len(numerical_features)],
        'Mean HAD (False Positives)': mean_had_fp
    })

    # Sort by the highest HAD or feature weight for interpretation
    results = results.sort_values(by='Mean HAD (False Positives)', ascending=False)

    return results

results = calculate_error_analysis(X_val_boruta_reduced, y_val, y_pred, notes_evaluated_pipeline)

def plot_top_had_features(results_df, num_features=10):
    """
    Plots the top features with the highest HAD values for false positives,
    overlaying feature weights to identify conflicting signals.

    Args:
        results_df (pd.DataFrame): DataFrame with 'Feature', 'Feature Weight',
                                     and 'Mean HAD (False Positives)' columns.
        num_features (int, optional): Number of top features to display.
                                        Defaults to 10.
    """

    # Sort by HAD values and select top features
    top_features = results_df.sort_values(
        by='Mean HAD (False Positives)', ascending=False
    ).head(num_features)
    print(top_features)

    # Create the bar plot
    plt.figure(figsize=(12, 6))
    sns.barplot(
        data=top_features, x='Feature', y='Mean HAD (False Positives)', color='skyblue'
    )
    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability
    plt.ylabel('Mean HAD (False Positives)')
    plt.title('Top Features with Highest HAD Values for False Positives')

    # Overlay feature weights as a line plot
    ax2 = plt.twinx()  # Create a secondary y-axis
    sns.lineplot(
        data=top_features, x='Feature', y='Feature Weight', ax=ax2, color='red', marker='o'
    )
    ax2.set_ylabel('Feature Weight')

    plt.tight_layout()
    plt.show()

plot_top_had_features(results, num_features=10)

def correlation_analysis(X_test, feature_name, target_feature):
    """
    Analyze correlations between the target feature and other features.
    """
    numerical_features = X_test.select_dtypes(include=np.number).columns
    X_test_numerical = X_test[numerical_features]

    correlations = X_test_numerical.corr()[feature_name].sort_values(ascending=False)
    print(f"Top correlations with {feature_name}:\n", correlations.head(10))
    print(f"Lowest correlations with {feature_name}:\n", correlations.tail(10))

# Example usage
correlation_analysis(X_val_boruta_reduced, 'Max Arterial base excess', 'Max Arterial base excess')

# Add range arterial base excess feature
X_train_boruta_reduced_range = X_train_boruta_reduced.copy()
X_train_boruta_reduced_range['Range Arterial base excess'] = X_train_boruta_reduced_range['Max Arterial base excess'] - X_train_boruta_reduced_range['Min Arterial base excess']
X_train_boruta_reduced_range = X_train_boruta_reduced_range.drop(columns=['Max Arterial base excess', 'Min Arterial base excess'])
X_val_boruta_reduced_range = X_val_boruta_reduced.copy()
X_val_boruta_reduced_range['Range Arterial base excess'] = X_val_boruta_reduced_range['Max Arterial base excess'] - X_val_boruta_reduced_range['Min Arterial base excess']
X_val_boruta_reduced_range = X_val_boruta_reduced_range.drop(columns=['Max Arterial base excess', 'Min Arterial base excess'])

# Run logistic regression with new range feature
range_evaluated_pipeline_range, range_balanced_acc_cv_range, range_pr_auc_cv_range, range_f1_cv_range, range_thresh = process_and_evaluate_model(
    X_train_boruta_reduced_range, y_train, X_val_boruta_reduced_range, y_val, recall_weight=5.0
)

y_pred_range = smote_evaluated_pipeline_range.predict(X_cv_boruta_reduced_range)

# Correlation Heatmap
def plot_correlation_heatmap(X_val, features):
    """
    Plots a correlation heatmap for the specified features in the dataset.

    Args:
        X_val (pd.DataFrame): Dataset with features.
        features (list of str): List of feature names to include in the correlation heatmap.
    """
    corr_matrix = X_val[features].corr()

    plt.figure(figsize=(8, 6))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', cbar=True, fmt='.2f', linewidths=0.5)
    plt.title("Correlation Heatmap", fontsize=16)
    plt.show()

# Plot the correlation heatmap
plot_correlation_heatmap(X_val_boruta_reduced, ['Max Arterial base excess', 'Mean Arterial base excess', 'Min Arterial base excess', 'Max pCO2_missing'])

def process_and_evaluate_rf_model(X, y, X_val=None, y_val=None):
    """
    Implements Random Forest and performs 5-fold cross-validation.
    Optionally evaluates the model on a validation set if provided.
    """
    X = X.copy()
    if X_val is not None:
        X_val = X_val.copy()
    X['Notes'] = X['Notes'].fillna('')
    if X_val is not None:
        X_val['Notes'] = X_val['Notes'].fillna('')

    numerical_columns = X.drop(columns=['Notes']).columns.tolist()
    text_column = 'Notes'

    numeric_transformer = Pipeline(steps=[
        ('scaler', StandardScaler()),
        ('imputer', SimpleImputer(strategy='mean'))
    ])
    text_transformer = Pipeline(steps=[
        ('tfidf', TfidfVectorizer(max_features=500))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numerical_columns),
            ('text', text_transformer, text_column)
        ]
    )

    # Random Forest Hyperparameter Grid
    param_distributions = {
        'classifier__n_estimators': [100, 200],
        'classifier__max_depth': [10, 30, None],
        'classifier__min_samples_split': [2, 5],
        'classifier__min_samples_leaf': [2, 4],
    }

    pipeline = ImbPipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', RandomForestClassifier(
            class_weight='balanced', random_state=42
        ))
    ])

    # 5-Fold Cross-Validation with Hyperparameter Tuning
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    randomized_search = RandomizedSearchCV(
        pipeline,
        param_distributions=param_distributions,
        n_iter=20,
        cv=skf,
        scoring='average_precision',
        n_jobs=-1,
        random_state=42,
        verbose=2
    )

    print("Performing hyperparameter tuning with SMOTE...")
    randomized_search.fit(X, y)

    best_pipeline = randomized_search.best_estimator_
    print(f"Best Parameters: {randomized_search.best_params_}")

    # Evaluate the tuned model using cross-validation predictions
    y_pred_cv = cross_val_predict(best_pipeline, X, y, cv=skf, method='predict')
    y_pred_prob_cv = cross_val_predict(best_pipeline, X, y, cv=skf, method='predict_proba')[:, 1]

    balanced_acc_cv = balanced_accuracy_score(y, y_pred_cv)
    precision_cv, recall_cv, _ = precision_recall_curve(y, y_pred_prob_cv)
    pr_auc_cv = auc(recall_cv, precision_cv)
    f1_cv = f1_score(y, y_pred_cv)

    print("\nCross-Validation Metrics:")
    print(f"Balanced Accuracy: {balanced_acc_cv:.4f}")
    print(f"PR AUC: {pr_auc_cv:.4f}")
    print(f"F1-Score: {f1_cv:.4f}")

    if X_val is not None and y_val is not None:
        print("\nEvaluating on the validation set...")
        best_pipeline.fit(X, y)
        y_val_pred = best_pipeline.predict(X_val)
        y_val_pred_prob = best_pipeline.predict_proba(X_val)[:, 1]

        balanced_acc_val = balanced_accuracy_score(y_val, y_val_pred)
        precision_val, recall_val, _ = precision_recall_curve(y_val, y_val_pred_prob)
        pr_auc_val = auc(recall_val, precision_val)
        f1_val = f1_score(y_val, y_val_pred)

        print("\Validation Set Metrics:")
        print(classification_report(y_val, y_val_pred))
        print(f"Balanced Accuracy: {balanced_acc_val:.4f}")
        print(f"PR AUC: {pr_auc_val:.4f}")
        print(f"F1-Score: {f1_val:.4f}")
        print(confusion_matrix(y_val, y_val_pred))

        return best_pipeline, balanced_acc_cv, pr_auc_cv, f1_cv, (balanced_acc_val, pr_auc_val, f1_val)

    return best_pipeline, balanced_acc_cv, pr_auc_cv, f1_cv, None

rf_evaluated_pipeline, rf_balanced_acc_val, rf_pr_auc_val, rf_f1_val, stats = process_and_evaluate_rf_model(X_train_boruta_reduced, y_train, X_val_boruta_reduced, y_val)

def process_and_evaluate_xgb_model(X, y, X_val=None, y_val=None):
    """
    Implements XGBoost and performs 5-fold cross-validation.
    Optionally evaluates the model on a validation set if provided.
    """
    X = X.copy()
    if X_val is not None:
        X_val = X_val.copy()

    # Handle missing values in text column
    X['Notes'] = X['Notes'].fillna('')
    if X_val is not None:
        X_val['Notes'] = X_val['Notes'].fillna('')

    # Split numerical and text columns
    numerical_columns = X.drop(columns=['Notes']).columns.tolist()
    text_column = 'Notes'

    # Define transformers
    numeric_transformer = Pipeline(steps=[
        ('scaler', StandardScaler()),
        ('imputer', SimpleImputer(strategy='mean'))
    ])
    text_transformer = Pipeline(steps=[
        ('tfidf', TfidfVectorizer(max_features=500))
    ])

    # Combine transformers
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numerical_columns),
            ('text', text_transformer, text_column)
        ]
    )

    # Define the full pipeline
    pipeline = ImbPipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', xgb.XGBClassifier(
            scale_pos_weight=5,
            eval_metric='logloss',
            random_state=42
        ))
    ])

    print("Performing 5-fold cross-validation...")
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    y_pred_cv = cross_val_predict(pipeline, X, y, cv=skf, method='predict')
    y_pred_prob_cv = cross_val_predict(pipeline, X, y, cv=skf, method='predict_proba')[:, 1]

    # Calculate metrics for cross-validation
    balanced_acc_cv = balanced_accuracy_score(y, y_pred_cv)
    precision_cv, recall_cv, _ = precision_recall_curve(y, y_pred_prob_cv)
    pr_auc_cv = auc(recall_cv, precision_cv)
    f1_cv = f1_score(y, y_pred_cv)

    print("\nCross-Validation Metrics:")
    print(f"Balanced Accuracy: {balanced_acc_cv:.4f}")
    print(f"PR AUC: {pr_auc_cv:.4f}")
    print(f"F1-Score: {f1_cv:.4f}")

    # If validation set is provided, evaluate separately
    if X_val is not None and y_val is not None:
        print("\nEvaluating on the validation set...")
        pipeline.fit(X, y)  # Train on the entire dataset
        y_val_pred = pipeline.predict(X_val)
        y_val_pred_prob = pipeline.predict_proba(X_val)[:, 1]

        # Calculate metrics for validation set
        balanced_acc_val = balanced_accuracy_score(y_val, y_val_pred)
        precision_val, recall_val, _ = precision_recall_curve(y_val, y_val_pred_prob)
        pr_auc_val = auc(recall_val, precision_val)
        f1_val = f1_score(y_val, y_val_pred)

        print("\Validation Set Metrics:")
        print(classification_report(y_val, y_val_pred))
        print(f"Balanced Accuracy: {balanced_acc_val:.4f}")
        print(f"PR AUC: {pr_auc_val:.4f}")
        print(f"F1-Score: {f1_val:.4f}")
        print(confusion_matrix(y_val, y_val_pred))

        return pipeline, balanced_acc_cv, pr_auc_cv, f1_cv, (balanced_acc_val, pr_auc_val, f1_val)

    return pipeline, balanced_acc_cv, pr_auc_cv, f1_cv, None

xgb_evaluated_pipeline, xgb_balanced_acc_cv, xgb_pr_auc_cv, xgb_f1_cv, stats = process_and_evaluate_xgb_model(X_train_boruta_reduced, y_train, X_val_boruta_reduced, y_val)

