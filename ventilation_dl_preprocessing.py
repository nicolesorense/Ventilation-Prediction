# -*- coding: utf-8 -*-
"""ventilation_dl_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12dDNoGNu2Jwx0sPs59whJRrtfum0vUTE

## Ventilation Prediction: Deep Learning Preprocessing
This notebook preprocesses MIMIC-IV time-series data from `ventilation_wrangling.py` for deep learning models to predict ventilation. It:
- Loads and cleans time-series CSVs.
- Handles missing values (static: mean/False, dynamic: forward-fill/group means, notes: empty string).
- Converts time-series features to lists, adds missingness indicators and time-since-last features.
- Standardizes features using MinMaxScaler.
- Combines features into 3D NumPy arrays
- Saves outputs as .npy files and notes as CSVs.

Install dependencies with: `pip install -r requirements.txt`
"""

import pandas as pd
import ast
import copy
import argparse
import os
import re
import numpy as np
from sklearn.model_selection import train_test_split
from oauth2client.client import GoogleCredentials
from sklearn.preprocessing import MinMaxScaler

def parse_args():
    parser = argparse.ArgumentParser(description="Preprocessing for deep learning.")
    parser.add_argument('--input_dir', type=str, default='data/wrangled/',
                       help='Directory containing wrangled MIMIC-IV CSV files.')
    parser.add_argument('--output_dir', type=str, default='data/wrangled/',
                       help='Directory to save preprocessed data for deep learning.')
    return parser.parse_args()

# For notebook, set manually (replace with parse_args() in .py)
args = type('Args', (), {
    'input_dir': 'data/wrangled/',
    'output_dir': 'data/wrangled/'
})()

# Load data
print("Warning: The input CSVs (train_time_series_vent_data.csv, test_time_series_vent_data.csv) contain MIMIC-IV-derived data, "
      "including sensitive clinical notes. Do not share publicly. Ensure compliance with the MIMIC-IV DUA.")
try:
    train_df = pd.read_csv(os.path.join(args.input_dir, 'train_time_series_vent_data.csv'))
    test_df = pd.read_csv(os.path.join(args.input_dir, 'test_time_series_vent_data.csv'))
except FileNotFoundError as e:
    print(f"Error: CSV file not found in {args.input_dir}: {e}")
    raise

# Drop irrelevant columns for prediction
train_df = train_df.drop(['Unnamed: 0', 'Unnamed: 0.1', 'subject_id', 'hadm_id', 'stay_id', 'first_careunit', 'last_careunit', 'intime', 'outtime', 'los', 'pred_window_start', 'pred_window_end', 'admittime', 'dischtime', 'curr_period_start', 'curr_period_end', 'period_index'], axis=1)
test_df = test_df.drop(['Unnamed: 0', 'Unnamed: 0.1', 'subject_id', 'hadm_id', 'stay_id', 'first_careunit', 'last_careunit', 'intime', 'outtime', 'los', 'pred_window_start', 'pred_window_end', 'admittime', 'dischtime', 'curr_period_start', 'curr_period_end', 'period_index'], axis=1)

# Drop original valuenum, charttime columns
cols_to_drop = [col for col in train_df.columns if 'valuenum' in col or 'charttime' in col]
train_df.drop(columns=cols_to_drop, inplace=True)
test_df.drop(columns=cols_to_drop, inplace=True)

# Split into validation and test sets at 1:1 ratio, stratify on ventilation
val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42, stratify=test_df['ventilation_label'])

# Reset indices
train_df = train_df.reset_index()
val_df = val_df.reset_index()
test_df = test_df.reset_index()

# For static variables, treat missing as false
train_df['has_respiratory_disease'] = train_df['has_respiratory_disease'].fillna(False)
train_df['has_hypertension'] = train_df['has_hypertension'].fillna(False)
train_df['has_neurological_disorder'] = train_df['has_neurological_disorder'].fillna(False)
train_df['has_cancer'] = train_df['has_cancer'].fillna(False)
train_df['has_heart_failure'] = train_df['has_heart_failure'].fillna(False)
train_df['has_diabetes'] = train_df['has_diabetes'].fillna(False)
train_df['has_renal_dysfunction'] = train_df['has_renal_dysfunction'].fillna(False)
train_df['Piperacillin'] = train_df['Piperacillin'].fillna(False)
train_df['Levofloxacin'] = train_df['Levofloxacin'].fillna(False)
train_df['Meropenem'] = train_df['Meropenem'].fillna(False)
train_df['Vancomycin'] = train_df['Vancomycin'].fillna(False)
val_df['has_respiratory_disease'] = val_df['has_respiratory_disease'].fillna(False)
val_df['has_hypertension'] = val_df['has_hypertension'].fillna(False)
val_df['has_neurological_disorder'] = val_df['has_neurological_disorder'].fillna(False)
val_df['has_cancer'] = val_df['has_cancer'].fillna(False)
val_df['has_heart_failure'] = val_df['has_heart_failure'].fillna(False)
val_df['has_diabetes'] = val_df['has_diabetes'].fillna(False)
val_df['has_renal_dysfunction'] = val_df['has_renal_dysfunction'].fillna(False)
val_df['Piperacillin'] = val_df['Piperacillin'].fillna(False)
val_df['Levofloxacin'] = val_df['Levofloxacin'].fillna(False)
val_df['Meropenem'] = val_df['Meropenem'].fillna(False)
val_df['Vancomycin'] = val_df['Vancomycin'].fillna(False)
test_df['has_respiratory_disease'] = test_df['has_respiratory_disease'].fillna(False)
test_df['has_hypertension'] = test_df['has_hypertension'].fillna(False)
test_df['has_neurological_disorder'] = test_df['has_neurological_disorder'].fillna(False)
test_df['has_cancer'] = test_df['has_cancer'].fillna(False)
test_df['has_heart_failure'] = test_df['has_heart_failure'].fillna(False)
test_df['has_diabetes'] = test_df['has_diabetes'].fillna(False)
test_df['has_renal_dysfunction'] = test_df['has_renal_dysfunction'].fillna(False)
test_df['Piperacillin'] = test_df['Piperacillin'].fillna(False)
test_df['Levofloxacin'] = test_df['Levofloxacin'].fillna(False)
test_df['Meropenem'] = test_df['Meropenem'].fillna(False)
test_df['Vancomycin'] = test_df['Vancomycin'].fillna(False)

"""## Time-Series Conversion
Converts time-series features (e.g., `Mean Heart Rate 0`, `Mean Heart Rate 1`, ...) into lists (e.g., `Mean Heart Rate: [72, 75, ...]`).
"""

def convert_columns_to_lists(df, time_step_pattern=r'_[0-9]+'):
    """
    Convert time series features from multiple columns to list format.

    Parameters:
    - df: Input DataFrame with time series columns (e.g., HeartRate_Mean_t0, HeartRate_Mean_t1).
    - time_step_pattern: Regex pattern to identify time step columns (default: '_t[0-9]+' for t0, t1, ...).

    Returns:
    - df_list: DataFrame with time series features as lists in single columns, preserving non-time-series columns.
    """
    # Identify time series variables (e.g., HeartRate_Mean, Lactate_Mean)
    ts_vars = set()
    for col in df.columns:
        if re.search(time_step_pattern, col):
            # Extract base feature name (e.g., HeartRate_Mean from HeartRate_Mean_t0)
            base_var = re.sub(time_step_pattern, '', col)
            ts_vars.add(base_var)

    # Create a new DataFrame for list format
    df_list = df.copy()

    # Convert each time series variable to a list
    for var in ts_vars:
        # Find all columns for this variable, sorted by time step
        matching_cols = sorted(
            [col for col in df.columns if col.startswith(var) and re.search(time_step_pattern, col)],
            key=lambda x: int(re.search(r'[0-9]+', x).group())  # Sort by time step number
        )
        if not matching_cols:
            continue

        # Create a list of values for each row
        df_list[var] = df[matching_cols].apply(lambda row: row.tolist(), axis=1)

        # Drop the original time series columns
        df_list = df_list.drop(columns=matching_cols)

    return df_list

train_df = convert_columns_to_lists(train_df)
val_df = convert_columns_to_lists(val_df)
test_df = convert_columns_to_lists(test_df)

def process_column(column):
    """
    Helper function to add missingness indicators and time since last measurement.
    """
    try:
        # Add a missing indicator column
        present_indicators = column.apply(
            lambda x: [0 if pd.isna(val) else 1 for val in x] if isinstance(x, list) else [1] * 6
        )

        # Add time since last measurement
        time_since_last = column.apply(
            lambda x: [None if pd.isna(val) else 0 for val in x] if isinstance(x, list) else [None] * 6
        )
        for i, lst in enumerate(present_indicators):
          if 1 not in list(lst):
            time_since_last[i] = [None, None, None, None, None, None]
          else:
            val_observed = False
            for j, val in enumerate(lst):
              if val == 1:
                time_since_last[i][j] = 0
                val_observed = True
              elif val_observed == False:
                time_since_last[i][j] = None
              else:
                time_since_last[i][j] = time_since_last[i][j-1] + 1

        # If list is empty, use None as placeholders
        column = column.apply(lambda x: [None]*6 if isinstance(x, list) and len(x) == 0 else x)
        present_indicators = present_indicators.apply(lambda x: [None]*6 if isinstance(x, list) and len(x) == 0 else x)
        time_since_last = time_since_last.apply(lambda x: [None]*6 if isinstance(x, list) and len(x) == 0 else x)

        return column, present_indicators, time_since_last
    except Exception as e:
        print(f"Error processing column: {e}")
        return column, None, None

def add_missing_indicators_and_time_since_last(df):
    """
    Add missing indicators and time since last measurement for dynamic variables.
    """
    for col in df.columns:
        # Check for time series cols
        if "[" in str(df[col][0]):
            try:
                processed_column, present_column, time_since_last_column = process_column(df[col])
                df[col] = processed_column
                if present_column is not None:
                    df[f'{col}_present'] = present_column
                if time_since_last_column is not None:
                    df[f'{col}_time_since_last'] = time_since_last_column
            except Exception as e:
                print(f"Error processing column '{col}': {e}")
    return df

train_df = add_missing_indicators_and_time_since_last(train_df)
val_df = add_missing_indicators_and_time_since_last(val_df)
test_df = add_missing_indicators_and_time_since_last(test_df)

# Drop cough effort, oxygen saturation, and albium (bc < 10% nonmissing values)
train_df = train_df.drop(['Cough Effort', 'Cough Effort_present', 'Cough Effort_time_since_last'], axis=1)
train_df = train_df.drop(['Oxygen Saturation', 'Oxygen Saturation_present', 'Oxygen Saturation_time_since_last'], axis=1)
train_df = train_df.drop(['Albumin', 'Albumin_present', 'Albumin_time_since_last'], axis=1)
val_df = val_df.drop(['Cough Effort', 'Cough Effort_present', 'Cough Effort_time_since_last'], axis=1)
val_df = val_df.drop(['Oxygen Saturation', 'Oxygen Saturation_present', 'Oxygen Saturation_time_since_last'], axis=1)
val_df = val_df.drop(['Albumin', 'Albumin_present', 'Albumin_time_since_last'], axis=1)
test_df = test_df.drop(['Cough Effort', 'Cough Effort_present', 'Cough Effort_time_since_last'], axis=1)
test_df = test_df.drop(['Oxygen Saturation', 'Oxygen Saturation_present', 'Oxygen Saturation_time_since_last'], axis=1)
test_df = test_df.drop(['Albumin', 'Albumin_present', 'Albumin_time_since_last'], axis=1)

# Remove bool vars with < 1% positive values
boolean_cols = train_df.columns[train_df.dtypes == 'bool']
for col in boolean_cols:
  if train_df[col].value_counts(normalize=True).min() < 0.01:
    print(col)
    train_df = train_df.drop(col, axis=1)
    val_df = val_df.drop(col, axis=1)
    test_df = test_df.drop(col, axis=1)

def forward_fill_helper(values):
  """
  Forward fills missing values for dynamic features and replaces
  leading missing values with the mean of the feature.
  """

  filled = []
  last_val = None

  for val in values:
    if val is not None:
      last_val = val
    filled.append(last_val if last_val is not None else val)
  return filled

def forward_fill_for_missing_values(df):
  dynamic_features = [col for col in df.columns if '[' in str(df[col].loc[0])]

  for col in dynamic_features:
    # Convert lists stored as strings to lists if necessary
    df[col] = df[col].apply(lambda x: ast.literal_eval(x.replace('nan', 'NaN', 'None')) if isinstance(x, str) else x)

    # Forward fill using the mean as the default
    df[col] = df[col].apply(lambda x: forward_fill_helper(x))

  return df

# Define bins and labels for age decades
# Include bins for <10 and >=70 to capture all patients
bins = [0, 10, 20, 30, 40, 50, 60, 70, 120]  # Extend to 120 for elderly patients
labels = ['0–9', '10–19', '20–29', '30–39', '40–49', '50–59', '60–69', '70+']

# Handle missing Age values (impute with median or drop)
train_df['Age'] = train_df['Age'].fillna(train_df['Age'].median())  # Or use another strategy

# Create Decade column
def add_decade_column(df, bins, labels):
  df['Decade'] = pd.cut(
      df['Age'],
      bins=bins,
      labels=labels,
      include_lowest=True,  # Include age 0 in 0–9 bin
      right=False  # Left-inclusive bins: [0, 10), [10, 20), etc.
  )
  return df

train_df = add_decade_column(train_df, bins, labels)
val_df = add_decade_column(val_df, bins, labels)
test_df = add_decade_column(test_df, bins, labels)

# Define grouping variable
GROUPING_COLUMNS = ['Decade']

def calculate_group_means(df, grouping_columns):
    """
    Calculate group means for time series features stored as lists.

    Parameters:
    - df: Input DataFrame with time series features as lists (e.g., HeartRate_Mean = [72, 75, 78]).
    - grouping_columns: List of columns to group by (e.g., ['Decade']).

    Returns:
    - group_means: Dict mapping feature to group key (tuple) to mean value.
    """
    group_means = {}
    global_means = {}

    # Identify time series columns (lists, excluding grouping columns and derived features)
    ts_columns = [
        col for col in df.columns
        if col not in grouping_columns
        and isinstance(df[col].iloc[0], (list, np.ndarray))
        and '_present' not in col
        and '_time_since_last' not in col
    ]

    for col in ts_columns:
        # Ensure lists (handle string lists from CSV)
        df[col] = df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

        # Create a DataFrame with exploded lists
        exploded = df[[col] + grouping_columns].explode(col)
        exploded[col] = pd.to_numeric(exploded[col], errors='coerce')  # Convert to float, handle NaN
        exploded = exploded.dropna(subset=[col])  # Remove NaN values

        # Calculate group means
        group_key_cols = grouping_columns
        grouped_means = exploded.groupby(group_key_cols)[col].mean().to_dict()
        group_means[col] = grouped_means

        # Calculate global mean
        global_mean = exploded[col].mean()
        global_means[col] = global_mean if not np.isnan(global_mean) else 0.0

        # Assign global mean for missing groups
        all_groups = df[group_key_cols].drop_duplicates().apply(tuple, axis=1)
        for group_key in all_groups:
            if group_key not in group_means[col]:
                group_means[col][group_key] = global_mean

    return group_means

def impute_initial_missing_values(df, group_means, grouping_columns):
    """
    Impute missing values in time series lists using group means.

    Parameters:
    - df: Input DataFrame with time series features as lists.
    - group_means: Dict from calculate_group_means (feature -> group key -> mean).
    - grouping_columns: List of columns to group by (e.g., ['Decade']).

    Returns:
    - df: DataFrame with imputed time series lists.
    """
    df = df.copy()  # Avoid modifying input DataFrame
    ts_columns = [
        col for col in df.columns
        if col in group_means
        and col not in grouping_columns
        and '_present' not in col
        and '_time_since_last' not in col
    ]

    for col in ts_columns:
        # Ensure lists
        df[col] = df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

        def impute_list(row):
            group_key = tuple(row[grouping_columns])
            group_mean = group_means[col].get(group_key, np.nan)
            if np.isnan(group_mean):
                return row[col]  # Skip if no valid mean
            return [round(group_mean, 2) if pd.isna(val) else round(val, 2) for val in row[col]]

        df[col] = df.apply(impute_list, axis=1)

    return df

group_means = calculate_group_means(train_df, GROUPING_COLUMNS)
train_imputed_df = impute_initial_missing_values(train_df, group_means, GROUPING_COLUMNS)
val_imputed_df = impute_initial_missing_values(val_df, group_means, GROUPING_COLUMNS)
test_imputed_df = impute_initial_missing_values(test_df, group_means, GROUPING_COLUMNS)

train_forward_filled_df = forward_fill_for_missing_values(train_imputed_df)
val_forward_filled_df = forward_fill_for_missing_values(val_imputed_df)
test_forward_filled_df = forward_fill_for_missing_values(test_imputed_df)

# Get lists of just dynamic and just static features
dynamic_features = [feature for feature in train_forward_filled_df.columns if '[' in str(train_forward_filled_df[feature].loc[0])]
static_features = [feature for feature in train_forward_filled_df.columns if '[' not in str(train_forward_filled_df[feature].loc[0]) and 'Notes' not in str(feature)]

def impute_static_features(df):
  """
  Impute missing values in static features using the mean.
  """
  for col in static_features:
    if df[col].isna().sum() > 0:
      df[col].fillna(df[col].mean(), inplace=True)
      # Use same mean to impute val and test sets
      val_forward_filled_df[col].fillna(df[col].mean(), inplace=True)
      test_forward_filled_df[col].fillna(df[col].mean(), inplace=True)
  return train_forward_filled_df
train_forward_filled_df = impute_static_features(train_forward_filled_df)

def handle_missing_notes(df):
  """
  Handle missing notes by replacing with empty string.
  """
  df['Notes'] = df['Notes'].fillna('')
  return df

train_forward_filled_df = handle_missing_notes(train_forward_filled_df)
val_forward_filled_df = handle_missing_notes(val_forward_filled_df)
test_forward_filled_df = handle_missing_notes(test_forward_filled_df)

def handle_missing_indicator_features(df):
  """
  Handle missing indicator features by replacing with 0.
  """
  present_features = [col for col in df.columns if '_present' in col]
  time_since_last_features = [col for col in df.columns if '_time_since_last' in col]
  for col in present_features:
    for i, lst in enumerate(df[col]):
      for j, val in enumerate(lst):
        if str(val) in (['None', 'NaN', 'nan']):
          df[col].loc[i][j] = 0
  for col in time_since_last_features:
    for i, lst in enumerate(df[col]):
      for j, val in enumerate(lst):
        if str(val) in (['None', 'NaN', 'nan']):
          df[col].loc[i][j] = -1
handle_missing_indicator_features(train_forward_filled_df)
handle_missing_indicator_features(val_forward_filled_df)
handle_missing_indicator_features(test_forward_filled_df)

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

def standardize_dynamic_features(train_df, val_df=None, test_df=None, dynamic_features=None):
    """
    Standardize time series features in train, val, and test DataFrames using training set parameters.

    Parameters:
    - train_df: Training DataFrame with time series features as lists.
    - val_df: Validation DataFrame (optional).
    - test_df: Test DataFrame (optional).
    - dynamic_features: List of time series feature columns (e.g., ['HeartRate_Mean', 'Lactate_Mean']).

    Returns:
    - train_df: Standardized training DataFrame.
    - val_df: Standardized validation DataFrame (if provided).
    - test_df: Standardized test DataFrame (if provided).
    - scalers: Dict of MinMaxScaler objects per feature.
    """
    train_df = train_df.copy()
    if val_df is not None:
        val_df = val_df.copy()
    if test_df is not None:
        test_df = test_df.copy()

    # Initialize scalers for each feature
    scalers = {}

    # Filter dynamic features
    dynamic_features = [
        col for col in dynamic_features
        if 'present' not in col and 'time_since_last' not in col
    ]

    # Standardize training set and fit scalers
    for col in dynamic_features:
        # Explode lists to compute min/max
        all_values = train_df[col].explode().dropna().astype(float).values.reshape(-1, 1)
        if len(all_values) == 0:
            print(f"Warning: No valid values for {col} in training set")
            continue

        # Fit scaler on training data
        scaler = MinMaxScaler(feature_range=(0, 1))
        scaler.fit(all_values)
        scalers[col] = scaler

        # Transform training set
        train_df[col] = train_df[col].apply(
            lambda x: scaler.transform(np.array(x, dtype=float).reshape(-1, 1)).flatten().tolist()
        )

    # Standardize validation set (if provided)
    if val_df is not None:
        for col in dynamic_features:
            if col in scalers:
                val_df[col] = val_df[col].apply(
                    lambda x: scalers[col].transform(np.array(x, dtype=float).reshape(-1, 1)).flatten().tolist()
                )

    # Standardize test set (if provided)
    if test_df is not None:
        for col in dynamic_features:
            if col in scalers:
                test_df[col] = test_df[col].apply(
                    lambda x: scalers[col].transform(np.array(x, dtype=float).reshape(-1, 1)).flatten().tolist()
                )

    return train_df, val_df, test_df, scalers

standardized_train_df, standardized_val_df, standardized_test_df, scalers = standardize_dynamic_features(
    train_forward_filled_df,
    val_forward_filled_df,
    test_forward_filled_df,
    dynamic_features
)

# Round values to 2 decimal places
for col in standardized_train_df.columns:
  if '[' in str(standardized_train_df[col].loc[0]) and 'present' not in col and 'time_since_last' not in col:
    standardized_train_df[col] = standardized_train_df[col].apply(lambda x: np.round(x, 2))
    standardized_val_df[col] = standardized_val_df[col].apply(lambda x: np.round(x, 2))
    standardized_test_df[col] = standardized_test_df[col].apply(lambda x: np.round(x, 2))

# Remove irrelevant features and ventilation
for var in ['index', 'ventilation_label', 'Decade']:
  static_features.remove(var)

def standardize_static_features(train_df, val_df=None, test_df=None, static_features=None):
    """
    Standardize static features in train, val, and test DataFrames using training set parameters.

    Parameters:
    - train_df: Training DataFrame with static features (e.g., Age, Gender).
    - val_df: Validation DataFrame (optional).
    - test_df: Test DataFrame (optional).
    - static_features: List of static feature columns (e.g., ['Age', 'Gender']).

    Returns:
    - train_df: Standardized training DataFrame.
    - val_df: Standardized validation DataFrame (if provided).
    - test_df: Standardized test DataFrame (if provided).
    - scaler: MinMaxScaler object fitted on training set.
    """
    train_df = train_df.copy()
    if val_df is not None:
        val_df = val_df.copy()
    if test_df is not None:
        test_df = test_df.copy()

    # Filter static features (exclude non-numeric or time series)
    static_features = [
        col for col in static_features
        if col in train_df.columns and not isinstance(train_df[col].iloc[0], (list, np.ndarray))
    ]

    # Encode Gender if present
    def encode_gender(df):
        if 'Gender' in df.columns:
            gender_map = {'F': 0, 'M': 1, 'Other': 0.5, np.nan: 0.5}  # Handle missing or other values
            df['Gender'] = df['Gender'].map(gender_map).fillna(0.5).astype(float)
        return df

    train_df = encode_gender(train_df)
    if val_df is not None:
        val_df = encode_gender(val_df)
    if test_df is not None:
        test_df = encode_gender(test_df)

    # Initialize scaler
    scaler = MinMaxScaler(feature_range=(0, 1))

    # Fit scaler on training set
    if static_features:
        train_data = train_df[static_features].dropna()
        if not train_data.empty:
            scaler.fit(train_data)
            # Transform training set
            train_df[static_features] = scaler.transform(train_df[static_features])
        else:
            print("Warning: No valid data for static features in training set")

    # Transform validation set (if provided)
    if val_df is not None and static_features:
        val_df[static_features] = scaler.transform(val_df[static_features])

    # Transform test set (if provided)
    if test_df is not None and static_features:
        test_df[static_features] = scaler.transform(test_df[static_features])

    return train_df, val_df, test_df, scaler

standardized_train_df, standardized_val_df, standardized_test_df, scalers = standardize_static_features(
    standardized_train_df,
    standardized_val_df,
    standardized_test_df,
    static_features
)

# Drop target column
train_vent_labels = standardized_train_df['ventilation_label']
standardized_train_df = standardized_train_df.drop(['ventilation_label'], axis=1)
val_vent_labels = standardized_val_df['ventilation_label']
standardized_val_df = standardized_val_df.drop(['ventilation_label'], axis=1)
test_vent_labels = standardized_test_df['ventilation_label']
standardized_test_df = standardized_test_df.drop(['ventilation_label'], axis=1)

def normalize_time_since_last(train_df, val_df=None, test_df=None, time_features=None, placeholder_value=-1):
    """
    Normalize 'time since last' features in train, val, and test DataFrames using training set parameters.

    Args:
        train_df (pd.DataFrame): Training DataFrame with 'time since last' features as lists.
        val_df (pd.DataFrame): Validation DataFrame (optional).
        test_df (pd.DataFrame): Test DataFrame (optional).
        time_features (list): List of column names representing 'time since last' features.
        placeholder_value (int): Value to represent missing or no prior measurement.

    Returns:
        train_df (pd.DataFrame): Normalized training DataFrame.
        val_df (pd.DataFrame): Normalized validation DataFrame (if provided).
        test_df (pd.DataFrame): Normalized test DataFrame (if provided).
        normalizers (dict): Dict of {feature: (min_val, max_val)} for each feature.
    """
    train_df = train_df.copy()
    if val_df is not None:
        val_df = val_df.copy()
    if test_df is not None:
        test_df = test_df.copy()

    # Initialize normalizers for each feature
    normalizers = {}

    # Filter time features
    time_features = [
        col for col in time_features
        if col in train_df.columns and isinstance(train_df[col].iloc[0], (list, np.ndarray))
    ]

    # Normalize training set and compute parameters
    for feature in time_features:
        # Extract valid values (exclude placeholders)
        col_values = train_df[feature].apply(lambda x: np.array(x, dtype=float) if isinstance(x, list) else np.nan)
        all_values = np.concatenate(col_values.dropna().values)
        valid_values = all_values[all_values != placeholder_value]

        if len(valid_values) == 0:
            print(f"Warning: No valid values for {feature} in training set")
            normalizers[feature] = (0, 1)  # Default min/max
            continue

        min_val, max_val = valid_values.min(), valid_values.max()
        if max_val == min_val:
            print(f"Warning: {feature} has constant valid values ({min_val})")
            normalizers[feature] = (min_val, min_val + 1)  # Avoid division by zero
        else:
            normalizers[feature] = (min_val, max_val)

        # Normalize training set
        train_df[feature] = train_df[feature].apply(
            lambda x: [
                (val - min_val) / (max_val - min_val) if val != placeholder_value else placeholder_value
                for val in x
            ] if isinstance(x, list) else x
        )

    # Normalize validation set (if provided)
    if val_df is not None:
        for feature in time_features:
            if feature in normalizers:
                min_val, max_val = normalizers[feature]
                val_df[feature] = val_df[feature].apply(
                    lambda x: [
                        (val - min_val) / (max_val - min_val) if val != placeholder_value else placeholder_value
                        for val in x
                    ] if isinstance(x, list) else x
                )

    # Normalize test set (if provided)
    if test_df is not None:
        for feature in time_features:
            if feature in normalizers:
                min_val, max_val = normalizers[feature]
                test_df[feature] = test_df[feature].apply(
                    lambda x: [
                        (val - min_val) / (max_val - min_val) if val != placeholder_value else placeholder_value
                        for val in x
                    ] if isinstance(x, list) else x
                )

    return train_df, val_df, test_df, normalizers

time_since_last_features = [col for col in standardized_train_df.columns if 'time_since_last' in col]

normalized_train_df, normalized_val_df, normalized_test_df, normalizers = normalize_time_since_last(
    standardized_train_df,
    standardized_val_df,
    standardized_test_df,
    time_since_last_features
)

# Convert _present and time_since_last features to numpy arrays
present_features = [col for col in normalized_train_df.columns if 'present' in col]
time_since_last_features = [col for col in normalized_train_df.columns if 'time_since_last' in col]
normalized_train_df[present_features] = normalized_train_df[present_features].map(lambda x: np.array(x) if isinstance(x, list) else x)
normalized_train_df[time_since_last_features] = normalized_train_df[time_since_last_features].map(lambda x: np.array(x) if isinstance(x, list) else x)
normalized_val_df[present_features] = normalized_val_df[present_features].map(lambda x: np.array(x) if isinstance(x, list) else x)
normalized_val_df[time_since_last_features] = normalized_val_df[time_since_last_features].map(lambda x: np.array(x) if isinstance(x, list) else x)
normalized_test_df[present_features] = normalized_test_df[present_features].map(lambda x: np.array(x) if isinstance(x, list) else x)
normalized_test_df[time_since_last_features] = normalized_test_df[time_since_last_features].map(lambda x: np.array(x) if isinstance(x, list) else x)

# Combine all dynamic features into a single 3D array
time_series_features = dynamic_features
train_time_series_data = np.stack(normalized_train_df[time_series_features].apply(lambda row: np.column_stack(row), axis=1).to_list())
val_time_series_data = np.stack(normalized_val_df[time_series_features].apply(lambda row: np.column_stack(row), axis=1).to_list())
test_time_series_data = np.stack(normalized_test_df[time_series_features].apply(lambda row: np.column_stack(row), axis=1).to_list())

# Expand static features to align with the time-series data
train_static_data = normalized_train_df[static_features].values
train_static_data = np.repeat(train_static_data[:, np.newaxis, :], 6, axis=1) # Repeat 6 times to match time-series data
val_static_data = normalized_val_df[static_features].values
val_static_data = np.repeat(val_static_data[:, np.newaxis, :], 6, axis=1) # Repeat 6 times to match time-series data
test_static_data = normalized_test_df[static_features].values
test_static_data = np.repeat(test_static_data[:, np.newaxis, :], 6, axis=1) # Repeat 6 times to match time-series data

# Combine static and dynamic features
train_combined_data = np.concatenate([train_time_series_data, train_static_data], axis=2)
val_combined_data = np.concatenate([val_time_series_data, val_static_data], axis=2)
test_combined_data = np.concatenate([test_time_series_data, test_static_data], axis=2)

def check_nan_inf_in_df(df):
    """
    Checks for NaN and Inf values in each column of a DataFrame.

    Args:
    - df (pd.DataFrame): The DataFrame to check.

    Returns:
    - problematic_columns (dict): A dictionary containing columns with NaN or Inf values and their counts.
    """
    problematic_columns = {}

    for col in df.columns:
        if isinstance(df[col].iloc[0], (list, np.ndarray)):  # Check for list-like or array-like elements
            # Flatten list values and check for NaN/Inf
            flattened = pd.DataFrame(df[col].to_list())
            nan_count = flattened.isna().sum().sum()
            inf_count = np.isinf(flattened).sum().sum()
        else:
            # Check for scalar values
            nan_count = df[col].isna().sum()
            # Apply replace element-wise using list comprehension for array-like elements
            if pd.api.types.is_numeric_dtype(df[col]):
                inf_count = np.isinf([x if not isinstance(x, (list, np.ndarray)) else
                                     np.isinf(np.array(x).astype(np.float64)).sum()
                                     for x in df[col]]).sum()
            else:
                inf_count = 0  # or handle non-numeric columns as needed

        if nan_count > 0 or inf_count > 0:
            problematic_columns[col] = {
                "NaN Count": nan_count,
                "Inf Count": inf_count
            }

    return problematic_columns

# Example: Check for NaN/Inf in the full DataFrame
problematic_columns = check_nan_inf_in_df(normalized_train_df)

# Print the problematic columns
if problematic_columns:
    print("Columns with NaN or Inf values:")
    for col, issues in problematic_columns.items():
        print(f"{col}: {issues}")
else:
    print("No NaN or Inf values found in the DataFrame.")

os.makedirs(args.output_dir, exist_ok=True)
print("Warning: Output .npy and CSV files contain MIMIC-IV-derived data, including sensitive clinical notes. Do not share publicly.")
np.save(os.path.join(args.output_dir, 'train_combined_time_series_data.npy'), train_combined_data)
np.save(os.path.join(args.output_dir, 'val_combined_time_series_data.npy'), val_combined_data)
np.save(os.path.join(args.output_dir, 'test_combined_time_series_data.npy'), test_combined_data)
np.save(os.path.join(args.output_dir, 'train_vent_labels.npy'), train_vent_labels.values)
np.save(os.path.join(args.output_dir, 'val_vent_labels.npy'), val_vent_labels.values)
np.save(os.path.join(args.output_dir, 'test_vent_labels.npy'), test_vent_labels.values)
normalized_train_df['Notes'].to_csv(os.path.join(args.output_dir, 'train_vent_notes.csv'))
normalized_val_df['Notes'].to_csv(os.path.join(args.output_dir, 'val_vent_notes.csv'))
normalized_test_df['Notes'].to_csv(os.path.join(args.output_dir, 'test_vent_notes.csv'))

